> 当前位置：【Java】09_Architecture_Distributed（分布式架构） -> 9.0_DistributedBasic（分布式架构基础）



# 第一章 分布式架构介绍

## 1、分布式系统概述

```
- 分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统
- 一个业务拆分成多个子业务，分布在不同的服务器节点，共同构成的系统称为分布式系统

特点：
- 同一个分布式系统中的服务器节点在空间部署上是可以随意分布的
- 这些服务器可能放在不同的机柜中，也可能在不同的机房中，甚至分布在不同的城市
```



## 2、分布式和集群

### （1）单机

![image-20210524133959650](image/image-20210524133959650.png)

### （2）集群

- 多个实例共同工作

![image-20210524134016818](image/image-20210524134016818.png)

### （3）分布式

- 把一个系统拆分为多个子系统，每个子系统负责各自的部分功能，独立部署，各司其职

![image-20210524134045907](image/image-20210524134045907.png)



- 分布式⼀定是集群，但是集群不⼀定是分布式

```
因为集群就是多个实例⼀起⼯作，分布式将⼀个系统拆分之后那就是多个实例；
集群并不⼀定是分布式，因为复制型的集群不是拆分⽽是复制
```



## 3、分布式系统特性

```
（1）分布性
- 空间中随机分布。这些计算机可以分布在不同的机房，不同的城市，甚至不同的国家

（2）对等性
- 分布式系统中的计算机没有主/从之分，组成分布式系统的所有节点都是对等的

（3）并发性
- 同一个分布式系统的多个节点，可能会并发地操作一些共享的资源，诸如数据库或分布式存储

（4）缺乏全局时钟
- 既然各个计算机之间是依赖于交换信息来进行相互通信，很难定义两件事件的先后顺序，缺乏全局始终控制序列

（5）故障总会发生
- 组成分布式的计算机，都有可能在某一时刻突然间崩掉
- 分的计算机越多，可能崩掉一个的几率就越大
- 如果再考虑到设计程序时的异常故障，也会加大故障的概率

（6）处理单点故障
- 单点SPOF（Single Point of Failure）：某个角色或者功能只有某一台计算机在支撑，在这台计算机上出现的故障是单点故障
```



## 4、分布式系统面临的问题

### 4.1 通信异常

```
- 网络本身的不可靠性，因此每次网络通信都会伴随着网络不可用的风险（光纤、路由、DNS等硬件设备或系统的不可用），都会导致最终分布式系统无法顺利进行一次网络通信

- 另外，即使分布式系统各节点之间的网络通信能够正常执行，其延时也会大于单机操作，存在巨大的延时差别，也会影响消息的收发过程，因此消息丢失和消息延迟变的非常普遍
```

![image-20210524134753958](image/image-20210524134753958.png)

### 4.2 网络分区

```
- 网络之间出现了网络不连通，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域，分布式系统就会出现局部小集群

- 在极端情况下，这些小集群会独立完成原本需要整个分布式系统才能完成的功能，包括数据的事务处理，这就对分布式一致性提出非常大的挑战
```

![image-20210524135059877](image/image-20210524135059877.png)

### 4.3 节点故障

```
- 节点故障是分布式系统下另一个比较常见的问题
- 指的是组成分布式系统的服务器节点出现的宕机或"僵死"现象
- 根据经验来说，每个节点都有可能出现故障，并且经常发生
```

![image-20210524135148132](image/image-20210524135148132.png)

### 4.4 三态

```
- 分布式系统每一次请求与响应存在特有的“三态”概念，即成功、失败和超时
```

![image-20210524135234979](image/image-20210524135234979.png)

### 4.5 重发

```
- 分布式系统在发生调用的时候可能会出现 失败或超时 的情况，这个时候需要重新发起调用
```

![image-20210524135340460](image/image-20210524135340460.png)

### 4.6 幂等

```
- 一次和多次请求某一个资源对于资源本身应该具有同样的结果（网络超时等问题除外）
- 任意多次执行对资源本身所产生的影响均与一次执行的影响相同
```

- 场景举例

```mysql
-- 无论执行多少次都不会改变状态，是天然的幂等
select * from users where id = 1;

-- 无论执行成功多少次，都是一致的，也是幂等
update users set age = 18 where id = 1;

-- 每次执行的结果都会发生变化，不是幂等
update users set age = age + 1 where id = 1;
```



# 第二章 分布式事务 - 理论

## 1、数据一致性

### 1.1 数据一致性概念

```
- 分布式数据一致性，指的是数据在多份副本中存储时，各副本中的数据是一致的
```

![image-20210524140816261](image/image-20210524140816261.png)



### 1.2 副本一致性

```
- 分布式系统中，数据往往会有多个副本，多个副本就需要保证数据的一致性，这就带来了同步的问题，因为网络延迟等因素, 我们几乎没有办法保证可以同时更新所有机器当中的包括备份所有数据，就会有数据不一致的情况

- 总得来说，我们无法找到一种能够满足分布式系统中数据一致性解决方案
- 因此，如何既保证数据的一致性，同时又不影响系统运行的性能，是每一个分布式系统都需要重点考虑和权衡的，于是一致性级别由此诞生
```

![image-20210524141812450](image/image-20210524141812450.png)



### 1.3 数据一致性分类

#### （1）强一致性

```
- 这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好
- 实现起来往往对系统的性能影响大
- 但是强一致性很难实现
```



#### （2）弱一致性

```
- 这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态
```



#### （3）最终一致性（弱一致性的一种）

```
- 它无法保证数据更新后，所有后续的访问都能看到最新数值，而是需要一个时间，在这个时间之后可以保证这一点（就是在一段时间后，节点间的数据会最终达到一致状态），而在这个时间内，数据也许是不一致的，这个系统无法保证强一致性的时间片段被称为不一致窗口

- 不一致窗口的时间长短取决于很多因素，比如备份数据的个数、网络传输延迟速度、系统负载等
```



#### （4）最终一致性的变种

##### 因果一致性

```
- 如果进程A通知进程B它已更新了一个数据项，那么进程B的后续访问将返回更新后的值
- 与进程A无因果关系的进程C的访问遵守一般的最终一致性规则
```

![image-20210524151559038](image/image-20210524151559038.png)

##### 读己之所写一致性

```
- 当进程A自己更新一个数据项之后，它总是访问到更新过的值，绝不会看到旧值。这是因果一致性模型的一个特例
```

![image-20210524152142698](image/image-20210524152142698.png)

##### 会话一致性

```
- 它把访问存储系统的进程放到会话的上下文中
- 只要会话还存在，系统就保证“读己之所写”一致性
- 如果由于某些失败情形令会话终止，就要建立新的会话，而且系统保证不会延续到新的会话
```

![image-20210524152309724](image/image-20210524152309724.png)

##### 单调读一致性

```
- 如果一个进程已经读取到一个特定值，那么该进程不会读取到该值以前的任何值
```

![image-20210524152719376](image/image-20210524152719376.png)

##### 单调写一致性

```
- 系统保证对同一个进程的写操作串行化
```

![image-20210524155227021](image/image-20210524155227021.png)



### 1.4 一致性模型图

![image-20210524155517888](image/image-20210524155517888.png)



## 2、CAP 定理（强一致性）

- CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer's theorem），它指出对于一个分布式算系统来说，不可能同时满足以下三点

| 选项                              | 具体意义                                                     |
| --------------------------------- | ------------------------------------------------------------ |
| 一致性（Consistency）             | 所有节点访问时都是同一份最新的数据副本                       |
| 可用性（Availability）            | 每次请求都能获取到非错的响应，但是不保证获取的数据为最新数据 |
| 分区容错性（Partition tolerance） | 分布式系统在遇到任何网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务，除非整个网络环境都发生了故障 |



### 2.1 一致性（Consistency）

```
- 这里指的是强一致性

- 在写操作完成后开始的任何读操作都必须返回该值，或者后续写操作的结果
- 也就是说，在一致性系统中，一旦客户端将值写入任何一台服务器并获得响应，那么之后client从其他任何服务器读取的都是刚写入的数据
```



![image-20210524160338490](image/image-20210524160338490.png)

```
（1）客户端向G1写入数据v1，并等待响应
（2）此时，G1服务器的数据为v1，而G2服务器的数据为v0，两者不一致
（3）接着，在返回响应给客户端之前，G2服务器会自动同步G1服务器的数据，使得G2服务器的数据也是v1
（4）一致性保证了不管向哪台服务器（比如这边向G1）写入数据，其他的服务器（G2）能实时同步数据
（5）G2已经同步了G1的数据，会告诉G1，我已经同步了
（6）G1接收了所有同步服务器的已同步的报告，才将“写入成功”信息响应给client
（7）client再发起请求，读取G2的数据
（8）此时得到的响应是v1，即使client读取数据到G2
```



### 2.2 可用性（A-Availability)

```
- 系统中非故障节点收到的每个请求都必须有响应
- 在可用系统中，如果我们的客户端向服务器发送请求，并且服务器未崩溃，则服务器必须最终响应客户端，不允许服务器忽略客户的请求
```



### 2.3 分区容错性（P-Partition tolerance）

```
- 允许网络丢失从一个节点发送到另一个节点的任意多条消息，即不同步
- 也就是说，G1和G2发送给对方的任何消息都是可以放弃的
- 也就是说G1和G2可能因为各种意外情况，导致无法成功进行同步，分布式系统要能容忍这种情况
```

<img src="image/image-20210524163006584.png" alt="image-20210524163006584"  />



### 2.4  CAP三者不可能同时满足论证

```
假设确实存在三者能同时满足的系统

（1）分区系统，由于满足分区容错性，也就是说可能因为通信不佳等情况，G1和G2之间是没有同步
（2）客户端将v1写入G1，但G1和G2之间是不同步的，所以如下G1是v1数据，G2是v0数据
（3）由于要满足可用性，即一定要返回数据，所以G1必须在数据没有同步给G2的前提下返回数据给 client
（4）client请求的是G2服务器，由于G2服务器的数据是v0，所以client得到的数据是v0
```

![image-20210524164502802](image/image-20210524164502802.png)

- 结论

```
- G1返回的是v1数据，G2返回的是v0数据，两者不一致。其余情况也有类似推导，也就是说 CAP三者不能同时出现。
```

![image-20210525143214280](image/image-20210525143214280.png)

### 2.5 CAP三者如何权衡

#### （1）CA (Consistency + Availability)

```
- 关注一致性和可用性，它需要非常严格的全体一致的协议
- CA 系统不能容忍网络错误或节点错误，一旦出现这样的问题，整个系统就会拒绝写请求，因为它并不知道对面的那个结点是否挂掉了，还是只是网络问题
- 唯一安全的做法就是把自己变成只读的
```

![image-20210525145043394](image/image-20210525145043394.png)

#### （2）CP (Consistency + Partition tolerance)

```
- 关注一致性和分区容忍性
- 它关注的是系统里大多数人的一致性协议
- 这样的系统只需要保证大多数结点数据一致，而少数的结点会在没有同步到最新版本的数据时变成不可用的状态，这样能够提供一部分的可用性
```

![image-20210525145157457](image/image-20210525145157457.png)

#### （3）AP (Availability + Partition tolerance)

```
- 关心可用性和分区容忍性
- 因此，这样的系统不能达成一致性，需要给出数据冲突，给出数据冲突就需要维护数据版本
```

![image-20210525145324024](image/image-20210525145324024.png)



### 2.6 场景选择

```
- 对于互联网应用来说，机器数量庞大，节点分散，网络故障再正常不过了，那么此时就是保障AP，放弃C的场景，而从实际中理解，像网站这种偶尔没有一致性是能接受的，但不能访问问题就非常大了

- 对于银行来说，就是必须保证强一致性，也就是说C必须存在，那么就只用CA和CP两种情况
（1）当保障强一致性和可用性（CA），那么一旦出现通信故障，系统将完全不可用
（2）另一方面，如果保障了强一致性和分区容错（CP），那么就具备了部分可用性
- 实际究竟应该选择什么，是需要通过业务场景进行权衡的（并不是所有情况都是CP好于CA，只能查看信息但不能更新信息有时候还不如直接拒绝服务）
```



## 3、BASE 理论（最终一致性）

```
BASE：
- Basically Available(基本可用)
- Soft state（软状态）
- Eventually consistent（最终一致性）
三个短语的缩写 

Base 理论是对 CAP 中一致性和可用性权衡的结果，其来源于对大型互联网分布式实践的总结，是基于 CAP 定理逐步演化而来的

核心思想： 即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）

原子性和持久性必须从根本上保障，为了可用性、性能和服务降级的需要，只有降低一致性和隔离性的要求
BASE 解决了 CAP 理论中没有考虑到的网络延迟问题，在BASE中用软状态和最终一致，保证了延迟后的一致性。
```



### 3.1  Basically Available（基本可用）

- 指的是基本业务可用性，支持分区失败

```
假设系统，出现了不可预知的故障，但还是能用，相比较正常的系统而言：
- 响应时间上的损失：正常情况下的搜索引擎 0.5 秒即返回给用户结果，而基本可用的搜索引擎可以在 1 秒返回结果
- 功能上的损失：在一个电商网站上，正常情况下，用户可以顺利完成每一笔订单，但是到了大促期间，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面
```



### 3.2 Soft state（软状态）

- 表示柔性状态，也就是允许短时间内不同步

```
- 硬状态：相对于原子性而言，要求多个节点的数据副本都是一致的
- 软状态：允许系统中的数据存在中间状态，并认为该状态不会影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时
```



### 3.3 Eventually consistent（最终一致性）

- 表示最终一致性，数据最终是一致的，但是实时是可以不一致的

```
- 上面说软状态，不可能一直是软状态，必须有个时间期限
- 在期限过后，应当保证所有副本保持数据一致性，从而达到数据的最终一致性
- 这个时间期限取决于网络延时、系统负载、数据复制方案设计等等因素
```



# 第三章 分布式事务 - 解决方案（一致性协议）

## 1、两阶段提交协议（2PC）- 强一致性

### 1.1 2PC 简介

```
- 两阶段提交协议，简称2PC(2 Prepare Commit)，是比较常用的解决分布式事务问题的方式
- 要么所有参与进程都提交事务，要么都取消事务，即实现ACID中的原子性(A)的常用手段

分布式事务: 事务提供一种操作本地数据库的不可分割的一系列操作 “要么什么都不做，要么做全套（All or Nothing）”的机制,而分布式事务就是为了操作不同数据库的不可分割的一系列操作 “要么什么都不做，要么做全套（All or Nothing）”的机制
```

![image-20210525150946619](image/image-20210525150946619.png)



### 1.2 2PC 执行流程

- 事务的发起者称协调者，事务的执行者称参与者
- 协调者统一协调参与者执行



#### 情形1：成功执行事务事务提交流程

![image-20210525151134768](image/image-20210525151134768.png)

```
阶段一：准备阶段
- 事务询问：协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应
- 执行事务：各参与者执行事务操作，但不提交事务，将 undo 和 redo 信息记入事务日志中
- 各参与者向协调者反馈事务询问的响应：如参与者执行成功，给协调者反馈 yes；如执行失败，给协调者反馈 no。

阶段二：提交阶段
- 发送提交请求：发送提交(commit)消息
- 事务提交：参与者收到 commit 请求后，会正式执行事务提交操作，并在完成提交之后释放整个事务执行期间占用的事务资源
- 反馈事务提交结果：参与者在完成事务提交之后，向协调者发送 Ack 信息
- 完成事务：协调者接收到所有参与者反馈的 Ack 信息后，完成事务
```



#### 情形2：中断事务流程

![image-20210525152131080](image/image-20210525152131080.png)

- 如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(rollback)消息

```
假如任何一个参与者向协调者反馈了No响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务

阶段一
- 事务询问：协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者的响应
- 执行事务 (写本地的Undo/Redo日志)
- 各参与者向协调者反馈事务询问的响应

阶段二
- 发送回滚请求：协调者向所有参与者发出 Rollback 请求
- 事务回滚：参与者接收到 Rollback 请求后，会利用其在阶段一中记录的 Undo 信息来执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源
- 反馈事务回滚结果：参与者在完成事务回滚之后，向协调者发送 Ack 信息
- 中断事务：协调者接收到所有参与者反馈的 Ack 信息后，完成事务中断
```



### 1.3 2PC 优缺点

```
优点：原理简单

缺点：
（1）同步阻塞（性能问题）
在二阶段提交的执行过程中，所有参与该事务操作的逻辑都处于阻塞状态，即当参与者占有共资源时，其他节点访问公共资源会处于阻塞状态，容易导致性能瓶颈。

（2）单点问题（可靠性问题）
若协调器出现问题，那么整个二阶段提交流程将无法运转，若协调者是在阶段二中出现问题时，那么其他参与者将会一直处于锁定事务资源的状态中，而无法继续完成事务操作

（3）数据不一致（数据一致性问题）
在阶段二中，执行事务提交的时候，当协调者向所有的参与者发送Commit请求之后，发生了局部网络异常或者是协调者在尚未发送完Commit请求之前自身发生了崩溃，导致最终只有部分参与者收到了Commit请求，于是会出现节点之间数据不一致的现象

（4）太过保守
在进行事务提交询问的过程中，参与者出现故障而导致协调者始终无法获取到所有参与者的响应信息的话，此时协调者只能依靠自身的超时机制来判断是否需要中断事务，这样的策略过于保守，即没有完善的容错机制，任意一个结点的失败都会导致整个事务的失败
```



## 2、三阶段提交协议（3PC）- 强一致性

### 2.1 3PC 简介

```
三阶段提交协议出现背景：
- 一致性协议中设计出了二阶段提交协议（2PC），但是2PC设计中还存在缺陷，于是就有了三阶段提交协议，这便是3PC的诞生背景

简介：
- 3PC，全称 “three phase commit”，是 2PC 的改进版，在协调者和参与者中都引入超时机制
- 将 2PC 的 “提交事务请求” 过程一分为二，共形成了由 CanCommit、PreCommit、doCommit 三个阶段组成的事务处理协议，
- 3PC协议并没有完全解决数据一致问题
```

![image-20210525153331788](image/image-20210525153331788.png)



### 2.2 第一阶段 CanCommit

```bash
# 类似于2PC的准备（第一）阶段。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应
- 事务询问：协调者向参与者发送CanCommit请求，询问是否可以执行事务提交操作。然后开始等待参与者的响应
- 响应反馈：参与者接到CanCommit请求后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态，否则反馈No
```



### 2.3 第二阶段 PreCommit

```bash
# 协调者根据参与者的反应情况来决定是否可以执行事务的PreCommit操作。根据响应情况，有以下两种可能

# 情形1：参与者均反馈 Yes
（1）发送预提交请求：协调者向参与者发送PreCommit请求，并进入Prepared阶段
（2）事务预提交：参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中
（3）响应反馈：如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令

# 情形2：任何一个参与者反馈 no或等待超时
假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。则有：
（1）发送中断请求：协调者向所有参与者发送abort请求
（2）中断事务: 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断
```



### 2.4 第三阶段 DoCommit

```bash
# 该阶段进行真正的事务提交，根据阶段 2 preCommit反馈的结果完成事务提交或中断操作，也可以分为执行提交和中断事务两种情况

# 执行成功
(1)发送提交请求: 协调者接收到参与者发送的ACK响应，那么它将从预提交状态进入到提交状态。 并向所有参与者发送doCommit请求
(2)事务提交: 参与者接收到doCommit请求之后，执行正式的事务提交。 并在完成事务提交之后释放所有事务资源
(3)响应反馈: 事务提交完之后，向协调者发送ACK响应
(4)完成事务: 协调者接收到所有参与者的ACK响应之后，完成事务

# 中断事务
(1)发送中断请求: 协调者向所有参与者发送abort请求
(2)事务回滚: 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源
(3)反馈结果: 参与者完成事务回滚之后，向协调者发送ACK消息
(4)中断事务: 协调者接收到所有参与者反馈的ACK消息之后，执行事务的中断
```

- 可能出现的2种故障

```
- 协调者出现问题
- 协调者和参与者之间的网络故障

如果出现了任一一种情况，最终都会导致参与者无法收到 doCommit 请求或者 abort 请求
针对这种情况，参与者都会在等待超时之后，继续进行事务提交
```

![image-20210525155059482](image/image-20210525155059482.png)



### 2.5 【区别】2PC 和 3PC

```
（1）首先对于协调者和参与者都设置了超时机制（在2PC中，只有协调者拥有超时机制，即如果在一定时间内没有收到参与者的消息则默认失败）,主要是避免了参与者在长时间无法与协调者节点通讯（协调者挂掉了）的情况下，无法释放资源的问题，因为参与者自身拥有超时机制会在超时后，自动进行本地commit从而进行释放资源。而这种机制也侧面降低了整个事务的阻塞时间和范围

（2）通过CanCommit、PreCommit、DoCommit三个阶段的设计，相较于2PC而言，多设置了一个缓冲阶段保证了在最后提交阶段之前各参与节点的状态是一致的

（3）PreCommit是一个缓冲，保证了在最后提交阶段之前各参与节点的状态是一致的
```



## 3、NWR 协议

### 3.1 简介

```
- NWR 是一种在分布式存储系统中用于控制一致性级别的一种策略
- 在亚马逊的云存储系统中，就应用 NWR 来控制一致性

N（node）：在分布式存储系统中，有多少份备份数据
W（write）：代表一次成功的更新操作要求至少有w份数据写入成功
R（read）：代表一次成功的读数据操作要求至少有R份数据成功读取
```

### 3.2 原理

```
- NWR值的不同组合会产生不同的一致性效果，当 W+R>N 的时候，整个系统对于客户端来讲能保证强一致性

以常见的N=3、W=2、R=2为例:
N=3表示，任何一个对象都必须有三个副本
W=2表示，对数据的修改操作只需要在3个副本中的2个上面完成就返回
R=2表示，从三个对象中要读取到2个数据对象，才能返回

在分布式系统中，数据的单点是不允许存在的。即线上正常存在的备份数量N设置1的情况是非常危险的，因为一旦这个备份发生错误，就 可能发生数据的永久性错误。假如我们把N设置成为2，那么，只要有一个存储节点发生损坏，就会有单点的存在。所以N必须大于2。N越高，系统的维护和整体 成本就越高。工业界通常把N设置为3
```

#### （1）当 W+R>N

```
- 当W是2、R是2的时候，W+R>N，这种情况对于客户端就是强一致性的，读取操作和写入操作成功的数据一定会有交集（如图中的Node2），这样就可以保证一定能够读取到最新版本的更新数据，数据的强一致性得到了保证

- 在满足数据一致性协议的前提下，R或者W设置的越大，则系统延迟越大，因为这取决于最慢的那份备份数据的响应时间
```



![image-20210525160822168](image/image-20210525160822168.png)

#### （2）当 R+W<=N

```
- 当 R+W<=N，无法保证数据的强一致性
- 因为成功写和成功读集合可能不存在交集，这样读操作无法读取到最新的更新数值，也就无法保证数据的强一致性
```

![image-20210525161004280](image/image-20210525161004280.png)

## 4、Gossip 协议

### 4.1 Gossip 概述

```
- Gossip 协议也叫 Epidemic 协议（流行病协议）。原本用于分布式数据库中节点同步数据使用，后被广泛用于数据库复制、信息扩散、集群成员身份确认、故障探测等。

- 从 gossip 单词就可以看到，其中文意思是八卦、流言等意思，我们可以想象下绯闻的传播（或者流行病的传播）；gossip 协议的工作原理就类似于这个。gossip 协议利用一种随机的方式将信息传播到整个网络中，并在一定时间内使得系统内的所有节点数据一致。Gossip 其实是一种去中心化思路的分布式协议，解决状态在集群中的传播和状态一致性的保证两个问题
```

### 4.2 Gossip  原理

- Gossip 协议的消息传播方式有两种：反熵传播 和 谣言传播

#### （1）反熵传播

```
- 是以固定的概率传播所有的数据
- 所有参与节点只有两种状态：Suspective(病原)、Infective(感染)
- 过程：种子节点会把所有的数据都跟其他节点共享，以便消除节点之间数据的任何不一致，它可以保证最终、完全的一致
- 缺点：消息数量非常庞大，且无限制；
- 通常只用于新加入节点的数据初始化
```

#### （2）谣言传播

```
- 是以固定的概率仅传播新到达的数据。
- 所有参与节点有三种状态：Suspective(病原)、Infective(感染)、Removed(愈除)
- 过程：消息只包含最新 update，谣言消息在某个时间点之后会被标记为 removed，并且不再被传播
- 缺点：系统有一定的概率会不一致
- 通常用于节点间数据增量同步
```



### 4.3 通信方式

- Gossip 协议最终目的是将数据分发到网络中的每一个节点。根据不同的具体应用场景，网络中两个节点之间存在三种通信方式：推送模式、拉取模式、推/拉模式

#### （1）Push

```
- 节点 A 将数据 (key,value,version) 及对应的版本号推送给 B 节点，B 节点更新 A 中比自己新的数据
```

![image-20210527101054909](image/image-20210527101054909.png)

#### （2）Pull

```
- A 仅将数据 key, version 推送给 B，B 将本地比 A 新的数据（Key, value, version）推送给 A，A 更新本地
```

![image-20210527101118000](image/image-20210527101118000.png)

#### （3）Push/Pull

```
- 与 Pull 类似，只是多了一步，A 再将本地比 B 新的数据推送给 B，B 则更新本地
```



### 4.4 优缺点

```
综上所述，可以得出 Gossip 是一种去中心化的分布式协议，数据通过节点像病毒一样逐个传播。因为是指数级传播，整体传播速度非常快。

1. 优点
- 扩展性：允许节点的任意增加和减少，新增节点的状态 最终会与其他节点一致
- 容错：任意节点的宕机和重启都不会影响 Gossip 消息的传播，具有天然的分布式系统容错特性
- 去中心化：无需中心节点，所有节点都是对等的，任意节点无需知道整个网络状况，只要网络连通，任意节点可把消息散播到全网
- 最终一致性：Gossip 协议实现信息指数级的快速传播，因此在有新信息需要传播时，消息可以快速地发送到全局节点，在有限的时间内能够做到所有节点都拥有最新的数据。

2. 缺点
- 消息延迟:节点随机向少数几个节点发送消息，消息最终是通过多个轮次的散播而到达全网；不可避免的造成消息延迟。
- 消息冗余:节点定期随机选择周围节点发送消息，而收到消息的节点也会重复该步骤；不可避免的引起同一节点消息多次接收，增加消息处理压力

Gossip 协议由于以上的优缺点，所以适合于 AP 场景的数据一致性处理，常见应用有：P2P 网络通信、Redis Cluster、Consul。
```



## 5、Paxos 协议

### 5.1 Paxos  概述

```
Paxos协议其实说的就是Paxos算法, Paxos算法是基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一。
```

### 5.2 Paxos  引入

```
在常见的分布式系统中，总会发生诸如机器宕机或网络异常（包括消息的延迟、丢失、重复、乱序，还有网络分区）等情况。Paxos算法需要解决的问题就是如何在一个可能发生上述异常的分布式系统中，快速且正确地在集群内部对某个数据的值达成一致，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性。

注：这里某个数据的值并不只是狭义上的某个数，它可以是一条日志，也可以是一条命令（command）。。。根据应用场景不同，某个数据的值有不同的含义
```

### 5.3 Paxos  解决2PC和3PC存在的问题

- 在之前讲解2PC 和 3PC的时候在一定程度上是可以解决数据一致性问题的. 但是并没有完全解决就是协调者宕机的情况

![image-20210527103058655](image/image-20210527103058655.png)

- 解决方法，步骤1：引入多个协调者

![image-20210527103226044](image/image-20210527103226044.png)

- 解决方法，步骤2：引入主协调者，主协调者负责协调，协调者负责跟随

![image-20210527103302495](image/image-20210527103302495.png)

```
其实在引入多个协调者之后又引入主协调者.那么这个就是最简单的一种Paxos 算法.

Paxos的版本有: 
- Basic Paxos
- Multi Paxos
- Fast-Paxos
具体落地有Raft 和zk的ZAB协议
```

![image-20210527103457548](image/image-20210527103457548.png)

### 5.4 Basic Paxos 相关概念

#### （1）角色介绍

```
Client:客户端
- 客户端向分布式系统发出请求，并等待响应。例如，对分布式文件服务器中文件的写请求。

Proposer：提案发起者
- 提案者提倡客户端请求，试图说服Acceptor对此达成一致，并在发生冲突时充当协调者以推动协议向前发展

Acceptor: 决策者，可以批准提案
- Acceptor可以接受（accept）提案；并进行投票, 投票结果是否通过以多数派为准, 以如果某个提案被选定，那么该提案里的value就被选定了

Learner: 最终决策的学习者
- 学习者充当该协议的复制因素(不参与投票)
```

#### （2）决策模型

![image-20210527103706060](image/image-20210527103706060.png)

#### （3）Basic Paxos 流程

```
步骤1：Prepare
- Proposer提出一个提案，编号为N，此N大于这个Proposer之前提出所有提出的编号，请求Accpetor的多数人接受这个提案

步骤2：Promise
如果编号N大于此Accpetor之前接收的任提案编号则接收，否则拒绝

步骤3：Accept
- 如果达到多数派，Proposer会发出accept请求，此请求包含提案编号和对应的内容

步骤4：Accepted
- 如果此Accpetor在此期间没有接受到任何大于N的提案,则接收此提案内容，否则忽略
```



### 5.5 Basic Paxos 流程图

#### （1）无故障的basic Paxos

![image-20210527103946640](image/image-20210527103946640.png)

#### （2）Acceptor失败时的basic Paxos

- 多数派中的一个Acceptor发生故障，因此多数派大小变为2。在这种情况下，Basic Paxos协议仍然成功

![image-20210527104016937](image/image-20210527104016937.png)

#### （3）Proposer失败时的basic Paxos

- Proposer在提出提案之后但在达成协议之前失败。具体来说，传递到Acceptor的时候失败了,这个时候需要选出新的Proposer（提案人）,那么 Basic Paxos协议仍然成功

![image-20210527104041850](image/image-20210527104041850.png)

#### （4）当多个提议者发生冲突时的basic Paxos

- 最复杂的情况是多个Proposer都进行提案,导致Paxos的活锁问题
- 针对活锁问题解决起来非常简单: 只需要在每个Proposer再去提案的时候随机加上一个等待时间即可

![image-20210527104059511](image/image-20210527104059511.png)



### 5.6 Multi-Paxos 流程图

- 针对 basic Paxos 是存在一定得问题，首先就是流程复杂，实现及其困难， 其次效率低（达成一致性需要2轮 RPC调用），针对 basic Paxos 流程进行拆分为选举和复制的过程

#### （1）第一次流程 - 确定Leader

![image-20211011162154406](image/image-20211011162154406.png)



#### （2）第二次流程 - 直接由Leader确认

![image-20211011164250845](image/image-20211011164250845.png)



### 5.7 Multi-Paxos 角色重叠流程图

- Multi-Paxos 在实施的时候会将Proposer、Acceptor、Learner的角色合并统称为“服务器”。因此，最后只有“客户端”和“服务器”。

![image-20211011164547057](image/image-20211011164547057.png)



## 6、Raft 协议

### 6.1 概述

- Paxos 是论证了一致性协议的可行性，但是论证的过程据说晦涩难懂，缺少必要的实现细节，而工程实现难度比较高, 广为人知实现只有 zk 的实现 zab 协议。
- Paxos协议的出现为分布式强一致性提供了很好的理论基础，但是Paxos协议理解起来较为困难，实现比较复杂。然后斯坦福大学RamCloud项目中提出了易实现，易理解的分布式一致性复制协议 Raft。Java，C++，Go 等都有其对应的实现之后出现的Raft相对要简洁很多。
- 引入主节点，通过竞选确定主节点（节点类型：Follower、Candidate、Leader）
  - Leader 会周期性的发送心跳包给 Follower
  - 每个 Follower 都设置了一个随机的竞选超时时间，一般为 150ms~300ms
  - 如果在这个时间内没有收到 Leader 的心跳包，就会变成 Candidate，进入竞选阶段，竞选阶段投票，多的人成为Leader



### 6.2 Raft 相关概念

#### （1）节点状态

- Leader（主节点）：接受 client 更新请求，写入本地后，然后同步到其他副本中
- Follower（从节点）：从 Leader 中接受更新请求，然后写入本地日志文件。对客户端提供读请求
- Candidate（候选节点）：如果 follower 在一段时间内未收到 leader 心跳。则判断 leader可能故障，发起选主提议。节点状态从 Follower 变为 Candidate 状态，直到选主结束

#### （2）termId

- 任期号，时间被划分成一个个任期，每次选举后都会产生一个新的 termId，一个任期内只有一个 leader。 

#### （3）RequestVote

- 请求投票，candidate 在选举过程中发起，收到多数派响应后，成为 leader。



### 6.3  竞选阶段流程

- Raft 动画演示：
  - http://thesecretlivesofdata.com/raft/
  - https://raft.github.io/

- 单节点是不存在数据不一致问题的，一个节点就很容易就该值达成一致性
- 如果是多个节点如何达成一致性 Raft 是用于实施分布式数据一致性协议的



Leader 节点宕机

多个 Candidate 竞选

日志复制

网络分区

网络分区情况下的日志复制



## 7、Lease 机制



## 8、XA 规范（强一致性，基于 2PC）

### 8.1 概述

- XA是由X/Open组织提出的分布式事务的规范，是基于两阶段提交协议。 
- XA规范主要定义了全局事务管理器（TM）和局部资源管理器（RM）之间的接口。
- 目前主流的关系型数据库产品都是实现了XA接口



### 8.2 全局事务DTP模型（标准分布式事务）

![image-20211130160433645](image/image-20211130160433645.png)

### 8.3 TM 事务管理器

- XA之所以需要引入事务管理器，是因为在分布式系统中，从理论上讲两台机器理论上无法达到一致的状态，需要引入一个单点进行协调。
- 由全局事务管理器管理和协调的事务，可以跨越多个资源（数据库）和进程。
- 事务管理器用来保证所有的事务参与者都完成了准备工作(第一阶段)。
- 如果事务管理器收到所有参与者都准备好的消息，就会通知所有的事务都可以提交了（第二阶段）。
- MySQL 在这个XA事务中扮演的是参与者的角色，而不是事务管理器。



## 9、TCC 模式（最终一致性）

- TCC（Try-Confifirm-Cancel）
- TCC 是服务化的两阶段编程模型，其 Try、Confifirm、Cancel 3 个方法均由业务编码实现

```bash
# 一阶段
Try 操作，负责资源的检查和预留；

# 二阶段，提交操作
Confirm 操作，执行真正的业务；
Cancel 是预留资源的取消；
```



## 【区别】TCC 和 XA

![image-20211130165541508](image/image-20211130165541508.png)

- TCC 模式相比于 XA，解决了如下几个缺点

```bash
解决了协调者单点，由主业务方发起并完成这个业务活动。业务活动管理器可以变成多点，引入集群。
同步阻塞：引入超时机制，超时后进行补偿，并且不会锁定整个资源，将资源转换为业务逻辑形式，粒度变小。
数据一致性，有了补偿机制之后，由业务活动管理器控制一致性。
```



## 10、消息队列模式（最终一致性，基于 TCC）

- 消息中间件可以基于 Kafka、RocketMQ 等消息队列。
- 核心：将分布式事务拆分成本地事务进行处理，将需要分布式处理的任务通过消息日志的方式来异步执行。消息日志可以存储到本地文本、数据库或MQ中间件，再通过业务规则人工发起重试。
- MQ 分布式事务处理流程

```bash
步骤1：事务主动方处理本地事务。事务主动方在本地事务中处理业务更新操作和MQ写消息操作。
步骤2：事务主动方通过消息中间件，通知事务被动方处理事务通知事务待消息。事务主动方主动写消息到MQ，事务消费方接收并处理MQ中的消息。
步骤3：事务被动方通过MQ中间件，通知事务主动方事务已处理的消息，事务主动方根据反馈结果提交或回滚事务。

# 为了数据的一致性，当流程中遇到错误需要重试，容错处理规则如下
当步骤 1 处理出错，事务回滚，相当于什么都没发生。
当步骤 2 处理出错，由于未处理的事务消息还是保存在事务发送方，可以重试或撤销本地业务操作。
如果事务被动方消费消息异常，需要不断重试，业务处理逻辑需要保证幂等。
如果是事务被动方业务上的处理失败，可以通过MQ通知事务主动方进行补偿或者事务回滚。
如果多个事务被动方已经消费消息，事务主动方需要回滚事务时需要通知事务被动方回滚。
```

![image-20211130165822167](image/image-20211130165822167.png)



## 11、Saga 模式（最终一致性）

### 11.1 概述

- 一个Saga事务是一个有多个短时事务组成的长时的事务。

-  在分布式事务场景下，把一个Saga分布式事务看做是一个由多个本地事务组成的事务，每个本地事务都有一个与之对应的补偿事务。在Saga事务的执行过程中，如果某一步执行出现异常，Saga事务会被终止，同时会调用对应的补偿事务完成相关的恢复操作，这样保证Saga相关的本地事务要么都是执行成功，要么通过补偿恢复成为事务执行之前的状态（自动反向补偿机制）

- Saga是一种补偿模式

  

### 11.2 Saga 事务基本协议

- 每个 Saga 事务由一系列幂等的有序子事务(sub-transaction) Ti 组成。 
- 每个 Ti 都有对应的幂等补偿动作 Ci，补偿动作用于撤销 Ti 造成的结果。



### 11.3 Saga 补偿策略

- 向前恢复（forward recovery）：对应于上面第一种执行顺序，发生失败进行重试，适用于必须要成功的场景。
- 向后恢复（backward recovery）：对应于上面提到的第二种执行顺序，发生错误后撤销掉之前所有成功的子事务，使得整个 Saga 的执行结果撤销。



### 11.4 Saga 执行顺序

#### 情形1：事务正常执行完成

- T1, T2, T3, ..., Tn
- 例如：减库存(T1)，创建订单(T2)，支付(T3)，依次有序完成整个事务

![image-20211130170454536](image/image-20211130170454536.png)

#### 情形2：事务回滚

- T1, T2, ..., Tj, Cj,..., C2, C1（其中 0 < j < n）
- 例如：减库存(T1)，创建订单(T2)，支付(T3)，支付失败，支付回滚(C3)，订单回滚(C2)，恢复库存(C1)。

![image-20211130170553865](image/image-20211130170553865.png)



## 12、Seata 框架

### 12.1 Seata 概述

- Seata（Simple Extensible Autonomous Transaction Architecture）是一套一站式分布式事务解决方案，
- Seata 是阿里集团和蚂蚁金服联合打造的分布式事务框架
- Seata目前的事务模式有AT、TCC、Saga、XA（默认是AT模式）



### 12.2 Seata AT 概述

- AT本质上是2PC协议的一种实现。
- Seata AT事务模型包含TM(事务管理器)，RM(资源管理器)，TC(事务协调器)

```bash
- TC是一个独立的服务需要单独部署
- TM和RM以jar包的方式同业务应用部署在一起，它们同TC建立长连接，在整个事务生命周期内，保持RPC通信
```



### 12.3 Seata AT 执行流程

- 全局事务的发起方作为 TM
- 全局事务的参与者作为 RM

```bash
# 第一阶段：各个阶段本地提交操作
- TM 开启分布式事务，负责全局事务的begin和commit/rollback（TM 向 TC 注册全局事务记录）；
- RM 作为参与者，负责分支事务的执行，并将结果上报，并且通过 TC 的协调进行 commit/rollback（RM 向 TC 汇报资源准备状态 ）；
- RM 分支事务结束，事务一阶段结束;

# 第二阶段：会根据第一阶段的情况决定是进行全局提交还是全局回滚操作
- 根据TC 汇总事务信息，由TM发起事务提交或回滚操作；
- TC 通知所有 RM 提交/回滚资源，事务二阶段结束；
```

![image-20211130171606235](image/image-20211130171606235.png)



# 第四章 分布式 - 设计策略

## 1、心跳检测

### 1.1 简介

```
在分布式环境中，存在非常多的节点（Node），如何检测一个节点出现了故障乃至无法工作了？
- 通常解决这一问题是采用心跳检测的手段，如同通过仪器对病人进行一些检测诊断一样

- 心跳顾名思义，就是以固定的频率向其他节点汇报当前节点状态的方式
- 收到心跳，一般可以认为一个节点和现在的网络是良好的
- 当然，心跳汇报时，一般也会携带一些附加的状态、元数据信息，以便管理
```

### 1.2 原理

![image-20210525161730662](image/image-20210525161730662.png)

```
若Server没有收到Node3的心跳时，Server认为Node3失联

但是失联是失去联系，并不确定是否是Node3故障，有可能是Node3处于繁忙状态，导致调用检测超时；也有可能是Server与Node3之间链路出现故障或闪断

所以心跳不是万能的，收到心跳可以确认节点正常，但是收不到心跳也不能认为该节点就已经宣告“死亡”

此时，可以通过一些方法帮助Server做决定： 周期检测心跳机制、累计失效检测机制
```

#### （1）周期检测心跳机制

```
Server端每间隔 t 秒向Node集群发起监测请求，设定超时时间，如果超过超时时间，则判断“死 亡”,可以把该节点踢出集群
```

![image-20210525161919090](image/image-20210525161919090.png)

#### （2）累计失效检测机制

```
- 在周期检测心跳机制的基础上，统计一定周期内节点的返回情况（包括超时及正确返回），以此计算节点的“死亡”概率
- 另外，对于宣告“濒临死亡”的节点可以发起有限次数的重试，以作进一步判断,如果超过次数则可以把该节点踢出集群
```

![image-20210525161944206](image/image-20210525161944206.png)

## 2、高可用

### 2.1 高可用HA设计

```
- 高可用(High Availability)是系统架构设计中必须考虑的因素之一,通常是指经过设计来减少系统不能提供服务的时间
- 系统高可用性的常用设计模式包括三种：主备（Master-SLave）、互备（Active-Active）和集群（Cluster）模式
```

| 可用性   | 一年中可故障时长 | 一天中可故障时长 |
| -------- | ---------------- | ---------------- |
| 90%      | 36.5天           | 144分钟          |
| 99%      | 3.6天            | 14.4分钟         |
| 99.9%    | 8.8小时          | 86.4秒           |
| 99.99%   | 52.6分钟         | 8.6秒            |
| 99.999%  | 5.3分钟          | 860毫秒          |
| 99.9999% | 31.5秒           | 86毫秒           |

#### （1）主备模式（Master-SLave）

```
主备模式就是Active-Standby模式，当主机宕机时，备机接管主机的一切工作，待主机恢复正常后，按使用者的设定以自动（热备）或手动（冷备）方式将服务切换到主机上运行。在数据库部分，习惯称之为MS模式。MS模式即Master/Slave模式，这在数据库高可用性方案中比较常用，如MySQL、Redis等就采用MS模式实现主从复制，保证高可用
```

![image-20210525163635473](image/image-20210525163635473.png)

#### （2）互备模式（Active-Active）

```
互备模式指两台主机同时运行各自的服务工作且相互监测情况。在数据库高可用部分，常见的互备是MM模式。MM模式即Multi-Master模式，指一个系统存在多个master，每个master都具有read-write能力，会根据时间戳或业务逻辑合并版本
```

![image-20210525163725663](image/image-20210525163725663.png)

#### （3）集群模式（Cluster）

```
- 集群模式是指有多个节点在运行，同时可以通过主控节点分担服务请求
- 集群模式需要解决主控节点本身的高可用问题，一般采用主备模式
```



### 2.2 高可用HA下"脑裂问题"

#### （1）简介

```
在高可用（HA）系统中，当联系两个节点的"心跳线"断开时(即两个节点断开联系时)，本来为一个整体、动作协调的HA系统，就分裂成为两个独立的节点(即两个独立的个体)。由于相互失去了联系，都以为是对方出了故障，两个节点上的HA软件像"裂脑人"一样，"本能"地争抢"共享资源"、争起"应用服务"。就会发生严重后果：

- 共享资源被瓜分、两边"服务"都起不来了；
- 两边"服务"都起来了，但同时读写"共享存储"，导致数据损坏（常见如数据库轮询着的联机日志出错）
```

#### （2）脑裂出现的原因

```
- 高可用服务器各节点之间心跳线链路发生故障，导致无法正常通信
- 因网卡及相关驱动坏了，ip配置及冲突问题（网卡直连）
- 因心跳线间连接的设备故障（网卡及交换机）
- 因仲裁的机器出问题（采用仲裁的方案）
- 高可用服务器上开启了iptables防火墙阻挡了心跳消息传输
- 高可用服务器上心跳网卡地址等信息配置不正确，导致发送心跳失败
- 其他服务配置不当等原因，如心跳方式不同，心跳广插冲突、软件Bug等
```

#### （3）脑裂预防方案

```
- 添加冗余的心跳线 [即冗余通信的方法]
同时用两条心跳线路 (即心跳线也HA)，这样一条线路坏了，另一个还是好的，依然能传送心跳消息，尽量减少"脑裂"现象的发生几率。

- 仲裁机制
当两个节点出现分歧时，由第3方的仲裁者决定听谁的。这个仲裁者，可能是一个锁服务，一个共享盘或者其它什么东西

- Lease机制

- 隔离(Fencing)机制
    共享存储fencing：确保只有一个Master往共享存储中写数据
    客户端fencing：确保只有一个Master可以响应客户端的请求
    Slave fencing：确保只有一个Master可以向Slave下发命令
```



## 3、容错性

### 3.1 简介

```
容错顾名思义就是IT系统对于错误包容的能力，容错的处理是保障分布式环境下相应系统的高可用或者健壮性

一个典型的案例就是对于缓存穿透问题的解决方案如图所示
```

![image-20210525165320727](image/image-20210525165320727.png)

### 3.2 问题描述

```
在项目中使用缓存通常都是先检查缓存中是否存在
如果存在直接返回缓存内容
如果不存在，就直接查询数据库然后再缓存查询结果返回

这个时候如果我们查询的某一个数据在缓存中一直不存在，就会造成每一次请求都查询DB，这样缓存就失去了意义，在流量大时，或者有人恶意攻击如频繁发起为id为“-1”的条件进行查询，可能DB就挂掉了
```

![image-20210525165538151](image/image-20210525165538151.png)

### 3.3 解决方案

```
（1）临时存放null值
（2）使用布隆过滤器
```

![image-20210525165721507](image/image-20210525165721507.png)

## 4、负载均衡

### 4.1 简介

```
负载均衡：其关键在于使用多台集群服务器共同分担计算任务，把网络请求及计算分配到集群可用的不同服务器节点上，从而达到高可用性及较好的用户操作体验

负载均衡器有硬件解决方案，也有软件解决方案
- 硬件解决方案有著名的F5
- 软件有LVS、HAProxy、Nginx等

如图，不同的用户client1、client2、client3访问应用，通过负载均衡器分配到不同的节点
```

![image-20210525170038069](image/image-20210525170038069.png)

### 4.2 Nginx 负载均衡的6种策略

| 方式               | 说明                                                         |
| ------------------ | ------------------------------------------------------------ |
| 轮询               | 默认方式，每个请求会按时间顺序逐一分配到不同的后端服务器     |
| weight             | 权重方式，在轮询策略的基础上指定轮询的几率,权重越大,接受请求越多 |
| ip_hash            | 依据ip分配方式,相同的客户端的请求一直发送到相同的服务器，以保证 |
| least_conn         | 最少连接方式,把请求转发给连接数较少的后端服务器              |
| fair（第三方）     | 响应时间方式,按照服务器端的响应时间来分配请求，响应时间短的优先分配 |
| url_hash（第三方） | 依据URL分配方式,按访问url的hash结果来分配请求，使每个url定向到同一 |



# 第五章 分布式 - 服务调用

## 1、服务调用实现方式

### 1.1  HTTP 应用协议的通信框架

#### （1）HttpURLConnection

```
java 原生 HttpURLConnection是基于http协议的，支持get，post，put，delete等各种请求方式，最常用的就是get和post
```

#### （2）Apache Common HttpClient

```
HttpClient 是Apache Common 下的子项目，可以用来提供高效的、最新的、功能丰富的支持HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本

- 实现了所有 HTTP 的方法（GET,POST,PUT,HEAD 等）
- 支持 HTTPS 协议
- 支持代理服务器等
```

#### （3）OKhttp3

```
OKHttp是一个当前主流的网络请求的开源框架, 用于替代HttpUrlConnection和Apache HttpClient

- 支持http2.0，对一台机器的请求共享一个socket
- 采用连接池技术，可以有效的减少Http连接数量
- 无缝集成GZIP压缩技术
- 支持Response Cache，避免重复请求
- 域名多IP支持
```

#### （4）RestTemplate

```
Spring RestTemplate 是 Spring 提供的用于访问 Rest 服务的客户端，RestTemplate 提供了多种便捷访问远程Http服务的方法，能够大大提高客户端的编写效率，所以很多客户端比如 Android或者第三方服务商都是使用 RestTemplate 请求 restful 服务

- 面向 URL 组件，必须依赖于主机 + 端口 + URI
- RestTemplate 不依赖于服务接口，仅关注 REST 响应内容
- Spring Cloud Feign
```



### 1.2 RPC框架

```
- RPC全称 remote procedure call，即远程过程调用
- 借助RPC可以做到像本地调用一样调用远程服务，是一种进程间的通信方式
```

#### （1）Java RMI

```
Java RMI（Romote Method Invocation）是一种基于Java的远程方法调用技术，是Java特有的一种RPC实现
```

![RPC框架 - Java RMI](image/RPC框架 - Java RMI.png)

#### （2）Hessian

```
- Hessian是一个轻量级的remoting onhttp工具，使用简单的方法提供了RMI的功能.
- 相比WebService，Hessian更简单、快捷
- 采用的是二进制RPC协议，因为采用的是二进制协议，所以它很适合于发送二进制数据
```

![RPC框架 - Hessian](image/RPC框架 - Hessian.png)

#### （3）Dubbo

```
- Dubbo是阿里巴巴公司开源的一个高性能优秀的服务框架
- 使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和Spring框架无缝集成

- Dubbo是一款高性能、轻量级的开源Java RPC框架
- 它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现
```

![image-20210526105315115](image/image-20210526105315115.png)

#### （4）gRPC

```
- gRPC是由Google公司开源的一款高性能的远程过程调用(RPC)框架，可以在任何环境下运行
- 该框架提供了负载均衡，跟踪，智能监控，身份验证等功能，可以实现系统间的高效连接
```

![image-20210526132327765](image/image-20210526132327765.png)

## 2、跨域调用

```
- 在分布式系统中，会有调用其他业务系统，导致出现跨域问题
- 跨域实质上是浏览器的一种保护处理，如果产生了跨域，服务器在返回结果时就会被浏览器拦截(注意：此时请求是可以正常发起的，只是
浏览器对其进行了拦截)，导致响应的内容不可用
```

### 2.1 产生跨域的几种情况

| 当前页面URL              | 被请求页面URL                   | 是否跨域 | 原因                         |
| ------------------------ | ------------------------------- | -------- | ---------------------------- |
| http://www.xxxx.com/     | http://www.xxxx.com/index.html  | 否       | 同源(协议, 域名, 端口号相同) |
| http://www.xxxx.com/     | https://www.xxxx.com/index.html | 跨域     | 协议不同(http/https)         |
| http://www.xxxx.com/     | http://www.baidu.com/           | 跨域     | 主域名不同(xxxx/baidu)       |
| http://www.xxxx.com/     | http://yyy.xxxx.com/            | 跨域     | 子域名不同(www/yyy)          |
| http://www.xxxx.com:8080 | http://www.xxxx.com:8090        | 跨域     | 端口号不同(8080/8090)        |

### 2.2 跨域解决方案

```java
（1）使用jsonp解决网站跨域
缺点：不支持post请求，代码书写比较复杂

（2）使用HttpClient内部转发

（3）使用设置响应头允许跨域
// 设置响应头允许跨域
response.setHeader(“Access-Control-Allow-Origin”, “*”); 

（4）基于Nginx搭建企业级API接口网关
    
（5）使用Zuul搭建微服务API接口网关
- 请求首先通过网关，进行路径的路由，定位到具体的服务节点上，然后使用zuul的过滤器的请求转发去解决跨域问题
- Zuul是spring cloud中的微服务网关
- 网关：是一个网络整体系统中的前置门户入口
```



# 第六章 分布式 - 服务治理

## 1、服务协调

### 1.1 概述

```
- 分布式协调技术主要用来解决分布式环境当中多个进程之间的同步控制，让他们有序的去访问某种临界资源，防止造成"脏数据"的后果
- 分布式锁是分布式协调技术实现的核心内容
```

![image-20210526142603204](image/image-20210526142603204.png)



### 1.2 分布式锁实现方式

#### （1）基于缓存（Redis等）实现分布式锁

```bash
- 获取锁的时候，使用setnx加锁，并使用expire命令为锁添加一个超时时间，超过该时间则自动释放锁，锁的value值为一个随机生成的UUID, 释放锁的时候进行判断
- 获取锁的时候还设置一个获取的超时时间，若超过这个时间则放弃获取锁
- 释放锁的时候，通过UUID判断是不是该锁，若是该锁，则执行delete进行锁释放

# 命令
# set一个key为value的字符串，返回1；若key存在，则什么都不做，返回0
SETNX

# 为key设置一个超时时间，单位为second，超过这个时间锁会自动释放，避免死锁
expire

# 删除key
delete
```



#### （2）基于ZooKeeper实现分布式锁

```
- ZooKeeper 是一个为分布式应用提供一致性服务的开源组件，内部是一个分层的文件系统目录树结构，规定同一个目录下只能有一个唯一文件名

基于ZooKeeper实现分布式锁的步骤如下：
- 创建一个目录mylock
- 线程A想获取锁就在mylock目录下创建临时顺序节点
- 获取mylock目录下所有的子节点，然后获取比自己小的兄弟节点，如果不存在，则说明当前线程顺序号最小，获得锁
- 线程B获取所有节点，判断自己不是最小节点，设置监听比自己次小的节点
- 线程A处理完，删除自己的节点，线程B监听到变更事件，判断自己是不是最小的节点，如果是则获得锁
```



## 2、服务削峰

### 2.1 为什么要削峰

```
主要是还是来自于互联网的业务场景，例如：
- 春节火车票抢购，大量的用户需要同一时间去抢购；
- 阿里双11秒杀， 短时间上亿的用户涌入，瞬间流量巨大（高并发）
```



### 2.2 流量削峰方案

```
削峰从本质上来说就是更多地延缓用户请求，以及层层过滤用户的访问需求，遵从“最后落地到数据库的请求数要尽量少”的原则
```

#### （1）消息队列解决削峰

```
要对流量进行削峰，最容易想到的解决方案就是：用消息队列来缓冲瞬时流量，把同步的直接调用转换成异步的间接推送，中间通过一个队列在一端承接瞬时的流量洪峰，在另一端平滑地将消息推送出去

- 消息队列中间件主要解决应用耦合，异步消息，流量削锋等问题
- 常用消息队列系统：目前在生产环境使用较多的消息队列有ActiveMQ、RabbitMQ、ZeroMQ、Kafka、RocketMQ 等
- 在这里，消息队列就像“水库”一样，拦截上游的洪水，削减进入下游河道的洪峰流量，从而达到减免洪水灾害的目的。
```

![image-20210526144335560](image/image-20210526144335560.png)

#### （2）流量削峰漏斗：层层削峰

```
- 分层过滤其实就是采用“漏斗”式设计来处理请求的，这样就像漏斗一样，尽量把数据量和请求量一层一层地过滤和减少了。
```

- 分层过滤的核心思想

```
- 通过在不同的层次尽可能地过滤掉无效请求
- 通过CDN过滤掉大量的图片，静态资源的请求
- 再通过类似Redis这样的分布式缓存过滤请求
```

- 分层过滤的基本原则

```
- 对写数据进行基于时间的合理分片，过滤掉过期的失效请求
- 对写请求做限流保护，将超出系统承载能力的请求过滤掉
- 涉及到的读数据不做强一致性校验，减少因为一致性校验产生瓶颈的问题
- 对写数据进行强一致性校验，只保留最后有效的数据
```

![image-20210526144604804](image/image-20210526144604804.png)



## 3、服务降级

### 3.1 服务降级概述

```
当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心服务正常运作或高效运作

整个架构整体的负载超出了预设的上限阈值或即将到来的流量预计将会超过预设的阈值时，为了保证重要或基本的服务能正常运行，将一些 不重要 或 不紧急 的服务或任务进行服务的 延迟使用或 暂停使用
```

![image-20210526145354391](image/image-20210526145354391.png)

### 3.2 降级策略

```
当触发服务降级后，新的交易再次到达时，该如何来处理这些请求呢？从分布式,微服务架构全局的视角来看，降级处理方案：
- 页面降级（可视化界面禁用点击按钮、调整静态页面）
- 延迟服务（如定时任务延迟处理、消息入MQ后延迟处理）
- 写降级（直接禁止相关写操作的服务请求）
- 读降级（直接禁止相关读的服务请求）
- 缓存降级（使用缓存方式来降级部分读频繁的服务接口）

针对后端代码层面的降级处理策略，则通常使用以下几种处理措施进行降级处理：
- 抛异常
- 返回NULL
- 调用Mock数据
- 调用Fallback处理逻辑
```

### 3.3 分级降级

- 结合服务能否降级的优先原则，并根据台风预警（都属于风暴预警）的等级进行参考设计，可将分布式服务架构的所有服务进行故障风暴等级划分为以下四种：

![image-20210526145810592](image/image-20210526145810592.png)



## 4、服务限流

### 4.1 服务限流概述

```
限流并非新鲜事，在生活中亦无处不在，下面例举一二：
- 博物馆：限制每天参观总人数以保护文物
- 地铁安检：有若干安检口，乘客依次排队，工作人员根据安检快慢决定是否放人进去。遇到节假日，可以增加安检口来提高处理能力，同时增加排队等待区长度。
- 水坝泄洪：水坝可以通过闸门控制泄洪速度

以上"限流"例子，可以让服务提供者稳定的服务客户
限流的目：通过对并发访问请求进行限速或者一个时间窗口内的的请求数量进行限速来保护系统，一旦达到限制速率则可以拒绝服务、排队或等待
```

![image-20210526150633329](image/image-20210526150633329.png)

### 4.2 多维度限流

```
在请求到达目标服务接口的时候, 可以使用多维度的限流策略,这样就可以让系统平稳度过瞬间来临的并发
```

![image-20210526150709973](image/image-20210526150709973.png)

### 4.3 限流算法

#### （1）计数器（固定窗口）

```
计数器限制每一分钟或者每一秒钟内请求不能超过一定的次数，在下一秒钟计数器清零重新计算

存在问题:
客户端在第一分钟的59秒请求100次，在第二分钟的第1秒又请求了100次, 2秒内后端会受到200次请求的压力，形成了流量突刺
```

![image-20210526151457479](image/image-20210526151457479.png)

#### （2）计数器（滑动窗口）

```
滑动窗口其实是细分后的计数器，它将每个时间窗口又细分成若干个时间片段，每过一个时间片段，整个时间窗口就会往右移动一格
时间窗口向右滑动一格，这时这个时间窗口其实已经打满了100次，客户端将被拒绝访问,时间窗口划分的越细，滑动窗口的滚动就越平滑，限流的效果就会越精确
```

![image-20210526151541157](image/image-20210526151541157.png)

#### （3）漏桶

```
漏桶算法类似一个限制出水速度的水桶，通过一个固定大小FIFO队列+定时取队列元素的方式实现，请求进入队列后会被匀速的取出处理（桶底部开口匀速出水），当队列被占满后后来的请求会直接拒绝（水倒的太快从桶中溢出来）

优点：可以削峰填谷，不论请求多大多快，都只会匀速发给后端，不会出现突刺现象，保证下游服务正常运行
缺点：在桶队列中的请求会排队，响应时间拉长
```

#### （4）令牌桶

```
令牌桶算法是以一个恒定的速度往桶里放置令牌（如果桶里的令牌满了就废弃），每进来一个请求去桶里找令牌，有的话就拿走令牌继续处理，没有就拒绝请求

优点：可以应对突发流量，当桶里有令牌时请求可以快速的响应，也不会产生漏桶队列中的等待时间
缺点：相对漏桶一定程度上减小了对下游服务的保护
```

![image-20210526152034202](image/image-20210526152034202.png)



## 5、服务熔断

### 5.1 熔断概述

```
- 熔断这一概念来源于电子工程中的断路器（Circuit Breaker）
- 在互联网系统中，当下游服务因访问压力过大而响应变慢或失败，上游服务为了保护系统整体的可用性，可以暂时切断对下游服务的调用。这种牺牲局部，保全整体的措施就叫做熔断
```



```
如果不采取熔断措施，我们的系统会怎样呢？
举例说明:
当前系统中有A，B，C三个服务，服务A是上游，服务B是中游，服务C是下游. 它们的调用链如下
```

![image-20210526161009694](image/image-20210526161009694.png)



```
一旦下游服务C因某些原因变得不可用，积压了大量请求，服务B的请求线程也随之阻塞
线程资源逐渐耗尽，使得服务B也变得不可用。紧接着，服务A也变为不可用，整个调用链路被拖垮，像这种调用链路的连锁故障，叫做雪崩
```

![image-20210526161044625](image/image-20210526161044625.png)



### 5.2 熔断机制

```
开启熔断
- 在固定时间窗口内，接口调用超时比率达到一个阈值，会开启熔断。
- 进入熔断状态后，后续对该服务接口的调用不再经过网络，直接执行本地的默认方法，达到服务降级的效果

熔断恢复
- 熔断不可能是永久的。当经过了规定时间之后，服务将从熔断状态回复过来，再次接受调用方的远程调用
```

![image-20210526161430870](image/image-20210526161430870.png)

​		

### 5.3 熔断机制实现

#### （1）Spring Cloud Hystrix

```
Spring Cloud Hystrix 是基于 Netflix 的开源框架 Hystrix 实现，该框架实现了服务熔断、线程隔离等一系列服务保护功能

对于熔断机制的实现，Hystrix设计了三种状态：
（1）熔断关闭状态（Closed）
- 服务没有故障时，熔断器所处的状态，对调用方的调用不做任何限制

（2）熔断开启状态（Open）
- 在固定时间内（Hystrix默认是10秒），接口调用出错比率达到一个阈值（Hystrix 默认为50%），会进入熔断开启状态。进入熔断状态后， 后续对该服务接口的调用不再经过网络，直接执行本地的 fallback 方法

（3）半熔断状态（Half-Open）
- 在进入熔断开启状态一段时间之后（Hystrix 默认是5秒），熔断器会进入半熔断状态
- 所谓半熔断就是尝试恢复服务调用，允许有限的流量调用该服务，并监控调用成功率。
- 如果成功率达到预期，则说明服务已恢复，进入熔断关闭状态；如果成功率仍旧很低，则重新进入熔断开启状态。
```

![image-20210526161825346](image/image-20210526161825346.png)

#### （2）Sentinel

```
Sentinel 和 Hystrix 的原则是一致的: 当调用链路中某个资源出现不稳定，例如，表现为 timeout，异常比例升高的时候，则对这个资源的调用进行限制，并让请求快速失败，防止避免影响到其它的资源，最终产生雪崩的效果。

Sentinel 熔断手段:
- 通过并发线程数进行限制
- 通过响应时间对资源进行降级
- 系统负载保护
```



## 6、服务链路追踪

### 6.1 链路追踪概述

```
分布式微服务架构上通过业务来划分服务的，通过REST调用对外暴露的一个接口，可能需要很多个服务协同才能完成这个接口功能，如果链路上任何一个服务出现问题或者网络超时，都会形成导致接口调用失败。随着业务的不断扩张，服务之间互相调用会越来越复杂。

分布式链路追踪（Distributed Tracing），也叫分布式链路跟踪，分布式跟踪，分布式追踪 等等. 其实就是将一次分布式请求还原成调用链路。显示的在后端查看一次分布式请求的调用情况，比如各个节点上的耗时、请求具体达到了哪台机器上、每个服务节点的请求状态等等
```



### 6.2 链路追踪具备的功能

#### （1）故障快速定位

```
通过调用链跟踪，一次请求的逻辑轨迹可以完整清晰的展示出来。开发中可以在业务日志中添加调用链ID，可以通过调用链结合业务日志快速定位错误信息
```

![image-20210526164656824](image/image-20210526164656824.png)

#### （2）各个调用环节的性能分析

```
在调用链的各个环节分别添加调用时延，可以分析系统的性能瓶颈，可以进行针对性的优化。通过分析各个环节的平均时延，QPS等信息，可以找到系统的薄弱环节，对一些模块做调整
```

![image-20210526164816935](image/image-20210526164816935.png)

#### （3）数据分析

```
调用链绑定业务后查看具体每条业务数据对应的链路问题，可以得到用户的行为路径，经过了哪些服务器上的哪个服务，汇总分析应用在很多业务场景。
```

#### （4）生成服务调用拓扑图

```
通过可视化分布式系统的模块和他们之间的相互联系来理解系统拓扑。点击某个节点会展示这个模块的详情，比如它当前的状态和请求数量
```



### 6.3 链路追踪设计原则

```
（1）设计目标
- 低侵入性，应用透明
- 低损耗
- 大范围部署，扩展性

（2）埋点和生成日志
- 埋点即系统在当前节点的上下文信息，可以分为客户端埋点、服务端埋点，以及客户端和服务端双向型埋点
- 埋点日志通常要包含以下内容：TraceId、RPCId、调用的开始时间、调用类型、协议类型、调用方ip和端口、请求的服务名等信息、调用耗时、调用结果、异常信息、消息报文等

（3）抓取和存储日志
- 日志的采集和存储有许多开源的工具可以选择，一般来说，会使用离线+实时的方式去存储日志，主要是分布式日志采集的方式。典型的解决方案如Flume结合Kafka。

（4）分析和统计调用链数据
- 一条调用链的日志散落在调用经过的各个服务器上，首先需要按 TraceId 汇总日志，然后按照 RpcId 对调用链进行顺序整理。调用链数据不要求百分之百准确，可以允许中间的部分日志丢失。

（5）计算和展示
- 汇总得到各个应用节点的调用链日志后，可以针对性的对各个业务线进行分析。需要对具体日志进行整理，进一步储存在HBase或者关系型数据库中，可以进行可视化的查询
```



### 6.4 链路追踪 Trace 模型

#### （1）概念

| 术语             | 解释                                                         |
| ---------------- | ------------------------------------------------------------ |
| Trace            | 一次完整的分布式调用跟踪链路                                 |
| Span             | 跟踪服务调用基本结构，表示跨服务的一次调用； 多span形成树形结构，组合成一次Trace追踪记录 |
| Annotation       | 在span中的标注点，记录整个span时间段内发生的事件：Cs CLIENT_SEND，客户端发起请求；Cr CLIENT_RECIEVE，客户端收到响应；Sr SERVER_RECIEVE，服务端收到请求；Ss SERVER_SEND，服务端发送结果 |
| BinaryAnnotation | 可以认为是特殊的Annotation，用户自定义事件：Event 记录普通事件；Exception 记录异常事件 |



#### （2）关系图

-  Client && Server：对于跨服务的一次调用，请求发起方为client，服务提供方为Server各术语在一次分布式调用中，关系如下图所示

![image-20210526170036282](image/image-20210526170036282.png)

#### （3）链路跟踪系统实现

```
大的互联网公司都有自己的分布式跟踪系统，比如Google的Dapper，Twitter的zipkin，淘宝的鹰眼，新浪的Watchman，京东的Hydra等等
```



# 第七章 分布式 - 网络通信

## 1、分布式架构网络通信 - 基本原理

- 要实现网络机器间的通讯，首先得来看看计算机系统网络通信的基本原理
- 在底层层面去看，网络通信需要做的就是将流从一台计算机传输到另外一台计算机，基于传输协议和网络IO来实现，其中传输协议比较出名的有tcp、udp等等

```java
- tcp、udp都是在基于Socket概念上为某类应用场景而扩展出的传输协议
- 网络IO，主要有bio、nio、aio三种方式
```

- 所有的分布式应用通讯都基于这个原理而实现



## 2、RPC（远程过程调用）

### 2.1 RPC 概述

- RPC（remote procedure call，即远程过程调用）
- 借助 RPC 可以做到像本地调用一样调用远程服务，是一种进程间的通信方式

```java
- 比如两台服务器 A 和 B，A 服务器上部署一个应用，B 服务器上部署一个应用，A 服务器上的应用想调用 B 服务器上的应用提供的方法，由于两个应用不在一个内存空间，不能直接调用，所以需要通过网络来表达调用的语义和传达调用的数据。
- 需要注意的是 RPC 并不是一个具体的技术，而是指整个网络远程调用过程。
```

- 本地调用

  ![image-20211008132931084](image/image-20211008132931084.png)

- RPC调用

![image-20211008133139126](image/image-20211008133139126.png)



### 2.2 RPC 架构核心组件

- 一个完整的RPC架构里面包含了四个核心的组件，分别是Client、Client Stub、Server、Server Stub

```java
- 客户端(Client)，服务的调用方。
- 客户端存根(Client Stub)，存放服务端的地址消息，再将客户端的请求参数打包成网络消息，然后通过网络远程发送给服务方。
- 服务端(Server)，真正的服务提供者。
- 服务端存根(Server Stub)，接收客户端发送过来的消息，将消息解包，并调用本地的方法。
```



### 2.3 RPC 调用过程

![image-20211008133346803](image/image-20211008133346803.png)

```java
1. 客户端（client）以本地调用方式（即以接口的方式）调用服务；
2. 客户端存根（client stub）接收到调用后，负责将方法、参数等组装成能够进行网络传输的消息体
（将消息体对象序列化为二进制）；
3. 客户端通过socket将消息发送到服务端；
4. 服务端存根( server stub）收到消息后进行解码（将消息对象反序列化）；
5. 服务端存根( server stub）根据解码结果调用本地的服务；
6. 服务处理
7. 本地服务执行并将结果返回给服务端存根( server stub）；
8. 服务端存根( server stub）将返回结果打包成消息（将结果消息对象序列化）；
9. 服务端（server）通过socket将消息发送到客户端；
10. 客户端存根（client stub）接收到结果消息，并进行解码（将结果消息发序列化）；
11. 客户端（client）得到最终结果。
         
- RPC的目标是要把2、3、4、5、7、8、9、10这些步骤都封装起来。只剩下1、6、11
注意：无论是何种类型的数据，最终都需要转换成二进制流在网络上进行传输，数据的发送方需要将对象转换为二进制流，而数据的接收方则需要把二进制流再恢复为对象。      
```



### 2.4 Java中的 RPC 框架

- 常见的有Hessian、gRPC、Dubbo 等
- 其实对于RPC框架而言，核心模块就是通讯和序列化



## 3、Java RMI（远程方法调用）

- Java RMI，即远程方法调用(Remote Method Invocation)，一种用于实现远程过程调用（RPCRemote procedure call）的Java API， 能直接传输序列化后的Java对象。它的实现依赖于Java虚拟机，因此它仅支持从一个JVM到另一个JVM的调用。

![image-20211008134137263](image/image-20211008134137263.png)



```
1.客户端从远程服务器的注册表中查询并获取远程对象引用。
2.桩对象与远程对象具有相同的接口和方法列表，当客户端调用远程对象时，实际上是由相应的桩对象代理完成的。
3.远程引用层在将桩的本地引用转换为服务器上对象的远程引用后，再将调用传递给传输层(Transport)，由传输层通过TCP协议发送调用；
4.在服务器端，传输层监听入站连接，它一旦接收到客户端远程调用后，就将这个引用转发给其上层的远程引用层； 
5.服务器端的远程引用层将客户端发送的远程应用转换为本地虚拟机的引用后，再将请求传递给骨架(Skeleton)； 
6.骨架读取参数，又将请求传递给服务器，最后由服务器进行实际的方法调用。
7.如果远程方法调用后有返回值，则服务器将这些结果又沿着“骨架->远程引用层->传输层”向下传递；
8.客户端的传输层接收到返回值后，又沿着“传输层->远程引用层->桩”向上传递，然后由桩来反序列化这些返回值，并将最终的结果传递给客户端程序。
```



# 第八章 分布式集群架构场景化解决⽅案

## 1、普通 Hash 算法

### 1.1 普通查找算法 - 分类

#### （1）顺序查找法

- 通过循环来完成

#### （2）二分查找法

- 排序之后折半查找

#### （3）直接寻址法

- 把数据和数的下标绑定到⼀起，查找的时候，array[n] 就取出了数据

- 优点：速度快，⼀次查找得到结果

- 缺点：浪费空间

  

### 1.2 普通 Hash 算法 - 分类

#### （1）除留余数法

- 对数据求模 （数据%空间位置数），这种构造Hash算法的⽅式叫做除留余数法

#### （2）开放寻址法

- 当有重复时，向前或者向后找空闲位置存放
- 缺点：数组长度固定，不能扩展

#### （3）拉链法

- 当有重复时，纵向放置一个链表



### 1.3 普通 Hash 算法 - 应用场景

#### （1）请求的负载均衡

- 如 Nginx 的 ip_hash 策略

```
Nginx的IP_hash策略可以在客户端ip不变的情况下，将其发出的请求始终路由到同⼀个⽬标服务器上，实现会话粘滞，避免处理session共享问题
- 可以对ip地址或者sessionid进⾏计算哈希值，哈希值与服务器数量进⾏取模运算，得到的值就是当前请求应该被路由到的服务器编号，如此，同⼀个客户端ip发送过来的请求就可以路由到同⼀个⽬标服务器，实现会话粘滞。

如果没有IP_hash策略，那么如何实现会话粘滞？
- 可以维护⼀张映射表，存储客户端IP或者sessionid与具体⽬标服务器的映射关系<ip,tomcat1>

如果没有IP_hash策略，缺点：
- 在客户端很多的情况下，映射表⾮常⼤，浪费内存空间
- 客户端上下线，⽬标服务器上下线，都会导致重新维护映射表，映射表维护成本很⼤
```



#### （2）分布式存储

- ⽐如分布式集群架构Redis、Hadoop、ElasticSearch，Mysql分库分表

```
以分布式内存数据库Redis为例,集群中有redis1，redis2，redis3 三台Redis服务器
- 在进⾏数据存储时,<key1,value1>数据存储到哪个服务器当中呢？
- 针对key进⾏hash处理 hash(key1)%3=index, 使⽤余数index锁定存储的具体服务器节点
```



### 1.4 普通 Hash 算法 - 存在的问题

- 服务器缩容和扩容，⼤量⽤户的请求会被路由到其他的⽬标服务器处理，⽤户在原来服务器中的会话都会丢失



## 2、一致性 Hash 算法

### 2.1 算法思路

#### （1）基本算法思路

- ⾸先有⼀条直线，直线开头和结尾分别定为为1和2的32次⽅减1，这相当于⼀个地址，对于这样⼀条线，弯过来构成⼀个圆环形成闭环，这样的⼀个圆环称为hash环。

- 把服务器的ip或者主机名求hash值然后对应到hash环上，那么针对客户端⽤户，也根据它的ip进⾏hash求值，对应到环上某个位置，然后如何确定⼀个客户端路由到哪个服务器处理呢？按照顺时针⽅向找最近的服务器节点

![image-20210910141230151](image/image-20210910141230151.png)

#### （2）缩容思路

- 假如将服务器3下线，服务器3下线后，原来路由到3的客户端重新路由到服务器4，对于其他客户端没有

  影响只是这⼀⼩部分受影响（请求的迁移达到了最⼩，这样的算法对分布式集群来说⾮常合适的，避免了⼤量请求迁移 ）

![image-20210910141642728](image/image-20210910141642728.png)

#### （3）扩容思路

- 如前所述，每⼀台服务器负责⼀段，⼀致性哈希算法对于节点的增减都只需重定位环空间中的⼀⼩部分数据，具有较好的容错性和可扩展性。
- 但是，⼀致性哈希算法在服务节点太少时，容易因为节点分部不均匀⽽造成数据倾斜问题。例如系统中只有两台服务器，其环分布如下，节点2只能负责⾮常⼩的⼀段，⼤量的客户端请求落在了节点1上，这就是数据（请求）倾斜问题

![image-20210913094142691](image/image-20210913094142691.png)

#### （4）虚拟节点（解决数据倾斜问题）

- 为了解决数据倾斜问题，⼀致性哈希算法引⼊了虚拟节点机制，即对每⼀个服务节点计算多个哈希，每个计算结果位置都放置⼀个此服务节点，称为虚拟节点。
- 具体做法可以在服务器ip或主机名的后⾯增加编号来实现。⽐如，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “节点1的ip#1”、“节点1的ip#2”、“节点1的ip#3”、“节点2的ip#1”、“节点2的ip#2”、“节点2的ip#3”的哈希值，于是形成六个虚拟节点，当客户端被路由到虚拟节点的时候其实是被路由到该虚拟节点所对应的真实节点

![image-20210913103411603](image/image-20210913103411603.png)

### 2.2 算法实现

#### （1）普通 Hash 算法实现

```java
public class a_GeneralHash {
    public static void main(String[] args) {
        // 定义客户端IP
        String[] clients = new String[]{"10.78.12.3", "113.25.63.1", "126.12.3.8"};

        // 定义服务器数量(编号对应0，1，2)
        int serverCount = 3;

        // index = hash(ip)%node_counts
        // 根据 index 锁定应该路由到的 tomcat 服务器
        for (String client : clients) {
            int hash = Math.abs(client.hashCode());
            int index = hash % serverCount;
            System.out.println("客户端：" + client + " 被路由到服务器编号为：" + index);
        }
    }
}
```

#### （2）一致性 Hash 算法实现

```java
public class b_ConsistentHash {
    public static void main(String[] args) {
        // 步骤1：初始化（把服务器节点IP的哈希值对应到哈希环上）
        // 定义服务器ip
        String[] tomcatServers = new String[]{"123.111.0.0", "123.101.3.1", "111.20.35.2", "123.98.26.3"};
        SortedMap<Integer, String> hashServerMap = new TreeMap<>();

        for (String tomcatServer : tomcatServers) {
            // 求出每一个ip的hash值，对应到hash环上
            int serverHash = Math.abs(tomcatServer.hashCode());
            // 存储hash值与ip的对应关系
            hashServerMap.put(serverHash, tomcatServer);
        }

        // 步骤2：针对客户端 IP 求出 hash 值
        // 定义客户端IP
        String[] clients = new String[]{"10.78.12.3", "113.25.63.1", "126.12.3.8"};
        for (String client : clients) {
            int clientHash = Math.abs(client.hashCode());

            // 步骤3：针对客户端，找到能够处理当前客户端请求的服务器（哈希环上顺时针最近）
            // 根据客户端ip的哈希值去找出哪一个服务器节点能够处理
            SortedMap<Integer, String> integerStringSortedMap = hashServerMap.tailMap(clientHash);
            if (integerStringSortedMap.isEmpty()) {
                // 如果为空，取哈希环上的顺时针第一台服务器
                Integer firstKey = hashServerMap.firstKey();
                System.out.println("==========>>>>客户端：" + client + " 被路由到服务器：" + hashServerMap.get(firstKey));
            } else {
                // 如果不为空，取哈希环的 integerStringSortedMap 中取第一个
                Integer firstKey = integerStringSortedMap.firstKey();
                System.out.println("==========>>>>客户端：" + client + " 被路由到服务器：" + hashServerMap.get(firstKey));
            }
        }
    }
}
```

#### （3）一致性 Hash 算法实现 - 带虚拟节点

```java
public class c_ConsistentHashWithVirtual {
    public static void main(String[] args) {
        // 步骤1：初始化（把服务器节点IP的哈希值对应到哈希环上）
        // 定义服务器ip
        String[] tomcatServers = new String[]{"123.111.0.0", "123.101.3.1", "111.20.35.2", "123.98.26.3"};
        SortedMap<Integer, String> hashServerMap = new TreeMap<>();

        // 定义针对每个真实服务器虚拟出来几个节点
        int virtualCount = 3;

        for (String tomcatServer : tomcatServers) {
            // 求出每一个ip的hash值，对应到hash环上
            int serverHash = Math.abs(tomcatServer.hashCode());
            // 存储hash值与ip的对应关系
            hashServerMap.put(serverHash, tomcatServer);

            // 处理虚拟节点
            for (int i = 0; i < virtualCount; i++) {
                int virtualHash = Math.abs((tomcatServer + "#" + i).hashCode());
                hashServerMap.put(virtualHash, "----由虚拟节点" + i + "映射过来的请求：" + tomcatServer);
            }
        }

        // 步骤2：针对客户端 IP 求出 hash 值
        // 定义客户端IP
        String[] clients = new String[]{"10.78.12.3", "113.25.63.1", "126.12.3.8"};
        for (String client : clients) {
            int clientHash = Math.abs(client.hashCode());

            // 步骤3：针对客户端，找到能够处理当前客户端请求的服务器（哈希环上顺时针最近）
            // 根据客户端ip的哈希值去找出哪一个服务器节点能够处理
            SortedMap<Integer, String> integerStringSortedMap = hashServerMap.tailMap(clientHash);
            if (integerStringSortedMap.isEmpty()) {
                // 如果为空，取哈希环上的顺时针第一台服务器
                Integer firstKey = hashServerMap.firstKey();
                System.out.println("==========>>>>客户端：" + client + " 被路由到服务器：" + hashServerMap.get(firstKey));
            } else {
                // 如果不为空，取哈希环的 integerStringSortedMap 中取第一个
                Integer firstKey = integerStringSortedMap.firstKey();
                System.out.println("==========>>>>客户端：" + client + " 被路由到服务器：" + hashServerMap.get(firstKey));
            }
        }
    }
}
```



## 3、集群时钟同步问题

### 3.1 时钟不同步导致的问题

- 时钟此处指服务器时间，如果集群中各个服务器时钟不⼀致导致的⼀系列问题

- 举⼀个例⼦，电商⽹站业务中，新增⼀条订单，那么势必会在订单表中增加了⼀条记录，该条记录中应该会有“下单时间”这样的字段，往往我们会在程序中获取当前系统时间插⼊到数据库或者直接从数据库服务器获取时间。那我们的订单⼦系统是集群化部署，或者我们的数据库也是分库分表的集群化部署，然⽽他们的系统时钟缺不⼀致，⽐如有⼀台服务器的时间是昨天，那么这个时候下单时间就成了昨天，那数据将会混乱！



### 3.2 集群时钟同步解决方案

#### （1）分布式集群中各个服务器节点都可以连接互联⽹

```bash
# 使⽤ ntpdate ⽹络时间同步命令，从⼀个时间服务器同步时间
ntpdate -u ntp.api.bz
```



#### （2）分布式集群中某⼀个服务器节点可以访问互联⽹或者所有节点都不能够访问互联⽹

- 选取集群中的⼀个服务器节点A（172.17.0.17）作为时间服务器，整个集群时间从这台服务器同步
- 如果这台服务器能够访问互联⽹，可以让这台服务器和⽹络时间保持同步
- 如果不能就⼿动设置⼀个时间

```bash
# 步骤 1：⾸先设置好A的时间

# 步骤 2：把A配置为时间服务器（修改/etc/ntp.conf⽂件）
# 如果有 restrict default ignore，注释掉它

# 添加如下⼏⾏内容
# 放开局域⽹同步功能，172.17.0.0是你的局域⽹⽹段
restrict 172.17.0.0 mask 255.255.255.0 nomodify notrap
server 127.127.1.0 # local clock
fudge 127.127.1.0 stratum 10

# 步骤 3：重启⽣效并配置ntpd服务开机⾃启动
service ntpd restart
chkconfig ntpd on

# 步骤 4：集群中其他节点从A服务器同步时间
ntpdate 172.17.0.17
```



## 4、分布式 ID 解决方案（主键生成策略）

### 4.1 UUID（可以用）

- UUID（Universally Unique Identififier，通⽤唯⼀识别码）

- 产⽣重复 UUID 并造成错误的情况⾮常低，是故⼤可不必考虑此问题
- Java中得到⼀个UUID，可以使⽤ java.util 包提供的⽅法

```java
public class a_UUID {
    public static void main(String[] args) {
        String uuid = UUID.randomUUID().toString();
        System.out.println(uuid);
    }
}
```



### 4.2 独立数据库的自增ID

- ⽐如A表分表为A1表和A2表，那么肯定不能让A1表和A2表的ID⾃增，那么ID怎么获取呢？
- 可以单独的创建⼀个Mysql数据库，在这个数据库中创建⼀张表，这张表的ID设置为⾃增，其他地⽅需要全局唯⼀ID的时候，就模拟向这个Mysql数据库的这张表中模拟插⼊⼀条记录，此时ID会⾃增，然后我们可以通过Mysql的select last_insert_id() 获取到刚刚这张表中⾃增⽣成的ID.

```sql
# 创建数据库
CREATE database if NOT EXISTS `JavaWeb_9.0_DistributedBasic` default character set utf8mb4 collate utf8mb4_unicode_ci;

# 选择数据库
use `JavaWeb_9.0_DistributedBasic`;

# 创建表
DROP TABLE IF EXISTS `DISTRIBUTE_ID`;
CREATE TABLE `DISTRIBUTE_ID`
(
    `id`         bigint(32) NOT NULL AUTO_INCREMENT COMMENT '主键',
    `createtime` datetime DEFAULT NULL,
    PRIMARY KEY (`id`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8;

# 插入初始数据
insert into DISTRIBUTE_ID(createtime) values (NOW());

# 使用：查询目前的可用的分布式 id
select LAST_INSERT_ID();

# 注意
- createtime字段⽆实际意义，是为了随便插⼊⼀条数据以⾄于能够⾃增id
- 使⽤独⽴的Mysql实例⽣成分布式id，虽然可⾏，但是性能和可靠性都不够好，因为你需要代码连接到数据库才能获取到id，性能⽆法保障，另外mysql数据库实例挂掉了，那么就⽆法获取分布式id了。
- 有⼀些开发者⼜针对上述的情况将⽤于⽣成分布式id的mysql数据库设计成了⼀个集群架构，其实这种⽅式现在基本不⽤，因为过于麻烦了。
```



### 4.3 SnowFlake 雪花算法（可以用，推荐）

- 雪花算法是⼀个算法，基于这个算法可以⽣成ID，⽣成的ID是⼀个long型
- 在Java中⼀个long型是8个字节，算下来是64bit
- 使⽤雪花算法⽣成的⼀个ID的⼆进制形式示意图

![image-20210913144433510](image/image-20210913144433510.png)

```
说明:
符号位: 固定为0，二进制表示最高位是符号位，0代表正数，1代表负数。
时间戳: 41个二进制数用来记录时间截，表示某一个毫秒 〈毫秒级)
机器id: 代表当前算法运行机器的id
序列号: 12位，用来记录某个机器同一个毫秒内产生的不同序列号，代表同一个机器同一个毫秒可以产生的ID序号。
```

- 基于上述的⽅案封装了⼀些分布式ID⽣成器
  - ⽐如滴滴的tinyid（基于数据库实现）
  - 百度的uidgenerator（基于SnowFlake）
  - 美团的leaf（基于数据库和SnowFlake）



### 4.4 借助Redis的Incr命令获取全局唯⼀ID（推荐）

- Redis Incr 命令将 key 中储存的数字值增⼀。如果 key 不存在，那么 key 的值会先被初始化为 0，然后再执⾏ INCR 操作。

#### （1）修改配置⽂件 redis.conf

```bash
# 步骤 1：注释
# bind 127.0.0.1 -::1

# 步骤 2：关掉保护模式
protected-mode no
```

#### （2）maven 添加依赖 - jedis

```xml
<dependency>
  <groupId>redis.clients</groupId>
  <artifactId>jedis</artifactId>
  <version>3.6.3</version>
</dependency>
```

#### （3）使用 jedis

```java
public static void main(String[] args) {
  Jedis jedis = new Jedis("192.168.137.145",6379);
  Long id = jedis.incr("id");//<id,0>
  System.out.println(id);
}
```



## 5、session 共享问题

### 5.1 产生原因

- 从根本上来说是因为Http协议是⽆状态的协议。
- 客户端和服务端在某次会话中产⽣的数据不会被保留下来，所以第⼆次请求服务端⽆法认识到你曾经来过， Http为什么要设计为⽆状态协议？早期都是静态⻚⾯⽆所谓有⽆状态，后来有动态的内容更丰富，就需要有状态，出现了两种⽤于保持Http状态的技术，那就是Cookie和Session。⽽出现上述不停让登录的问题



### 5.2 解决方案

#### （1）Nginx 的 IP_Hash 策略（可以使⽤）

- 同⼀个客户端IP的请求都会被路由到同⼀个⽬标服务器，也叫做会话粘滞

```
优点：
- 配置简单，不⼊侵应⽤，不需要额外修改代码

缺点：
- 服务器重启Session丢失
- 存在单点负载⾼的⻛险
- 单点故障问题
```



#### （2）Session复制（不推荐）

- 多个tomcat之间通过修改配置⽂件，达到Session之间的复制

```
优点：
- 不⼊侵应⽤
- 便于服务器⽔平扩展
- 能适应各种负载均衡策略
- 服务器重启或者宕机不会造成Session丢失

缺点：
- 性能低
- 内存消耗
- 不能存储太多数据，否则数据越多越影响性能
- 延迟性
```



#### （3）基于 Redis 的 Session共享（推荐）

- Session的本质就是缓存，将 Session 数据交给专业的缓存中间件 Redis，集中存储

```
优点:
- 能适应各种负载均衡策略
- 服务器重启或者宕机不会造成Session丢失
- 扩展能⼒强
- 适合⼤集群数量使⽤

缺点：
- 对应⽤有⼊侵，引⼊了和Redis的交互代码
```

 

##### 步骤 1：添加依赖

- Spring Session使得基于Redis的Session共享应⽤起来⾮常之简单

```xml

<!-- redis -->
<dependency>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter-data-redis</artifactId>
</dependency>

<!-- spring-session-data-redis -->
<dependency>
  <groupId>org.springframework.session</groupId>
  <artifactId>spring-session-data-redis</artifactId>
</dependency>
</dependencies>
```

##### 步骤 2：application.properties

```properties
# Redis 配置信息
spring.redis.database = 0
spring.redis.host = 127.0.0.1
spring.redis.port = 6379
```

##### 步骤 3：启动类添加注解 @EnableRedisHttpSession

```java
@SpringBootApplication
@EnableCaching
@EnableRedisHttpSession
public class LoginApplication extends SpringBootServletInitializer {
    @Override
    protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) {
        return builder.sources(LoginApplication.class);
    }

    public static void main(String[] args) {
        SpringApplication.run(LoginApplication.class, args);
    }
}
```

##### 步骤 4：Nginx 默认轮询策略



### 5.3 Spring session 原理

- 请求通过 tomcat 到达servlet容器的时候，通过过滤器对请求做了一次封装
- 如果没有过滤器，serlvet就会从 tomcat中获取Session
- 有了过滤器之后，取出来的Session就是Redis中的Session
- 总结：有过滤器就从Redis中获取，没有的话就创建并提交到Redis中去

![image-20210916091027143](image/image-20210916091027143.png)
