> 当前位置：【Java】09_Architecture_Distributed（分布式架构） -> 9.11_ElasticSearch（分布式搜索引擎）



# ElasticSearch 下载安装和配置

## 0、ElasticSearch 下载 

- 官网：https://www.elastic.co/cn/elasticsearch/
- 所有版本下载：https://www.elastic.co/cn/downloads/past-releases#elasticsearch
- 7.3.0下载：https://www.elastic.co/cn/downloads/past-releases/elasticsearch-7-3-0



## 1、Docker - ElasticSearch 安装和配置



## 2、Linux系统 - ElasticSearch 安装和配置

### 2.1 单机搭建

#### 步骤1：解压

```bash
tar -zxvf elasticsearch-7.3.0-linux-x86_64.tar.gz
mv /root/elasticsearch-7.3.0 /usr/elasticsearch/

cd /usr/elasticsearch/
```



#### 步骤2：修改配置文件

##### （1）elasticsearch.yml

```bash
vim /usr/elasticsearch/config/elasticsearch.yml

# 放开下面的注释
node.name: node-1
network.host: 192.168.203.133
http.port: 9200
cluster.initial_master_nodes: ["node-1"]
```

##### （2）jvm.options

```bash
vim /usr/elasticsearch/config/jvm.options

# 默认都是1G，单机1G内存，启动会占用700m+，然后在安装 kibana 后，基本上无法运行了，运行了一会就挂了报内存不足，根据实际情况修改占用内存
-Xms1g
-Xmx1g
```

##### （3）sysctl.conf

```bash
vi /etc/sysctl.conf

# 末尾添加
vm.max_map_count=655360

# 刷新生效
sysctl -p
```

##### （4）limits.conf

```bash
vim /etc/security/limits.conf

# 末尾添加
*                soft    nofile          65536 
*                hard    nofile          65536 
*                soft    nproc           4096 
*                hard    nproc           4096
```



#### 步骤3：添加 ES 用户

```bash
# 默认root用户无法启动，需要改为其他用户
# 添加用户
useradd estest

# 修改密码（此处密码设置为123）
passwd estest

# 改变es目录拥有者账号
chown -R estest /usr/elasticsearch/
```



#### 步骤4：启动 ES

```bash
# 切换命令
su estest

# 前台启动
/usr/elasticsearch/bin/elasticsearch

# 后台启动
cd /usr/elasticsearch/bin/
./elasticsearch -d

# 查看进程
ps aux | grep elasticsearch
```



#### 步骤5：访问

- http://192.168.203.133:9200/



### 2.2 集群搭建

#### 步骤1：配置 elasticsearch.yml

-  修改完配置文件之后，一定要把之前的data目录下node数据删除再重新服务即可

```yaml
# 集群名称
cluster.name: my-es

# 节点名称
node.name: node-1 
 
# 当前节点是否可以被选举为master节点，是：true、否：false
node.master: true 

network.host: 0.0.0.0 
http.port: 9200 
transport.port: 9300 

# 初始化一个新的集群时需要此配置来选举master 
cluster.initial_master_nodes: ["node-1","node-2","node-3"] 

# 写入候选主节点的设备地址
discovery.seed_hosts: ["127.0.0.1:9300", "127.0.0.1:9301","127.0.0.1:9302"] 

http.cors.enabled: true 
http.cors.allow-origin: "*"
```



#### 步骤2：第2节点配置

```bash
# 拷贝原来的 ES节点 elasticsearch 并命名为elasticsearch2，并授权
cp elasticsearch/ elasticsearch2 -rf
chown -R estest elasticsearch2

# 进入 elasticsearch2 目录config文件夹，修改elasticsearch.yml配置文件并保存。
node.name: node-2 
http.port: 9202 
transport.port: 9302

# 启动从环境2 一定要用estest用户来执行 
cd bin/ 
./elasticsearch
```



#### 步骤3：第3节点配置

```bash
# 拷贝原来的 ES节点 elasticsearch 并命名为elasticsearch3，并授权
cp elasticsearch/ elasticsearch3 -rf
chown -R estest elasticsearch3

# 进入 elasticsearch3 目录config文件夹，修改elasticsearch.yml配置文件并保存。
node.name: node-3 
http.port: 9203 
transport.port: 9303

# 启动从环境3 一定要用estest用户来执行 
cd bin/ 
./elasticsearch
```



#### 步骤4：验证

- http://192.168.203.133:9200/_cat/health?v



#### 步骤5： 安装 Elasticsearch Head 插件

- https://github.com/mobz/elasticsearch-head
- nodejs 安装

```bash
# 下载 
wget https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz 

# 解压
tar xf node-v10.15.3-linux-x64.tar.xz 

# 进入解压目录 
cd node-v10.15.3-linux-x64/ 

# 执行node命令
./bin/node -v 

# 查看版本 
v10.15.3

# 使用 ln 命令来设置软连接
ln -s /root/node-v10.15.3-linux-x64/bin/npm /usr/local/bin/ 
ln -s /root/node-v10.15.3-linux-x64/bin/node /usr/local/bin/
```

- phantomjs 安装

```bash
cd /usr/local 
wget https://github.com/Medium/phantomjs/releases/download/v2.1.1/phantomjs- 2.1.1-linux-x86_64.tar.bz2 

yum install -y bzip2 
tar -jxvf phantomjs-2.1.1-linux-x86_64.tar.bz2 

vim /etc/profile 
export PATH=$PATH:/usr/local/phantomjs-2.1.1-linux-x86_64/bin 

source /etc/profile
```

-  elasticsearch-head 安装

```bash
npm install -g grunt-cli 
npm install grunt 
npm install grunt-contrib-clean 
npm install grunt-contrib-concat 
npm install grunt-contrib-watch 
npm install grunt-contrib-connect 

yum -y install git 
git clone git://github.com/mobz/elasticsearch-head.git 

cd elasticsearch-head 
npm install -g cnpm --registry=https://registry.npm.taobao.org

# 设置跨域
http.cors.enabled: true 
http.cors.allow-origin: "*"

# 启动（在 elasticsearch-head 中执行命令）
npm run start
```

- 访问地址：http://192.168.203.133:9100



# Kibana 下载安装和配置

## 0、Kibana 下载

- 官网：https://www.elastic.co/cn/kibana/

- 所有版本：https://www.elastic.co/cn/downloads/past-releases#kibana
- 7.3.0版本：https://www.elastic.co/cn/downloads/past-releases/kibana-7-3-0



## 1、Docker - Kibana 安装和配置



## 2、Linux系统 - Kibana 安装和配置

### 步骤1：解压

```bash
# root账户下操作
tar -zxvf kibana-7.3.0-linux-x86_64.tar.gz
mv /root/kibana-7.3.0-linux-x86_64 /usr/kibana/
cd /usr/kibana/

# 改变es目录拥有者账号
chown -R estest /usr/kibana/

# 设置访问权限
chmod -R 777 /usr/kibana/
```



### 步骤2：修改配置文件

```bash
# 修改配置文件
vim /usr/kibana/config/kibana.yml

# 修改端口、访问ip、elasticsearch服务器ip
server.port: 5601 
server.host: "0.0.0.0" 
elasticsearch.hosts: ["http://192.168.203.133:9200"]
i18n.locale: "zh-CN"
```



### 步骤3：启动

```bash
# 切换用户
su estest 

# 前台启动
cd /usr/kibana
./bin/kibana

# 后台启动
cd /usr/kibana
nohup ./bin/kibana &

# 停止
netstat -tunlp|grep 5601
kill -9 xxx
```



### 步骤4：访问

- http://192.168.203.133:5601/app/kibana



# Elasticsearch 插件（IK 分词器）

## 1、IK 分词器

- IKAnalyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包。
- 从2006年12月推出1.0版开始，IKAnalyzer已经推出 了3个大版本
- 最初，它是以开源项目Lucene为应用主体的，结合词典分词和文法分析算法的中文分词组件
- 新版本的IKAnalyzer3.0则发展为 面向Java的公用分词组件，独立于Lucene项目，同时提供了对Lucene的默认优化实现。
- IK分词器 3.0 特性

```bash
- 采用了特有的“正向迭代最细粒度切分算法“，具有60万字/秒的高速处理能力。
- 采用了多子处理器分析模式，支持：英文字母（IP地址、Email、URL）、数字（日期，常用中文数量词，罗马数字，科学计数法），中文词汇（姓名、地名处理）等分词处理。
- 支持个人词条的优化的词典存储，更小的内存占用。
- 支持用户词典扩展定义。
- 针对Lucene全文检索优化的查询分析器IKQueryParser；采用歧义分析算法优化查询关键字的搜索排列组合，能极大的提高Lucene检索的命中率。
```



## 2、安装

### （1）安装方式1：命令下载

```bash
/usr/elasticsearch/bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.3.0/elasticsearch-analysis-ik-7.3.0.zip

# 下载完成后会提示 Continue with installation? 输入 y 即可完成安装
# 重启 Elasticsearch 和 Kibana
```

### （2）安装方式2：上传安装包

- https://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v7.3.0

```bash
# 在elasticsearch安装目录的plugins目录下
cd /usr/elasticsearch/plugins/

# 新建analysis-ik文件夹 
mkdir analysis-ik 

# 切换至 analysis-ik文件夹下 
cd analysis-ik 

# 上传资料中的 elasticsearch-analysis-ik-7.3.0.zip 

# 解压 
unzip elasticsearch-analysis-ik-7.3.3.zip 

# 解压完成后删除zip 
rm -rf elasticsearch-analysis-ik-7.3.0.zip

# 重启 Elasticsearch 和 Kibana
```



## 3、测试案例

```bash
# 分词模式：ik_max_word，将文本做最细粒度的拆分
POST _analyze
{
  "analyzer": "ik_max_word",
  "text": "南京市长江大桥"
}

# 分词模式：ik_smart，最粗粒度的拆分
POST _analyze
{
  "analyzer": "ik_smart",
  "text": "南京市长江大桥"
}
```



## 4、自定义词典

### 步骤1：切换目录

```bash
# 插件命令安装方式
cd /usr/elasticsearch/config/analysis-ik/

# 安装包安装方式
cd /usr/elasticsearch/plugins/analysis-ik/config
```



### 步骤2：新建自定义词典

```bash
# 扩展词典
vi td_ext_dict.dic

# 停用词典
vi td_stop_dict.dic


```



### 步骤3：修改配置文件 IKAnalyzer.cfg.xml

- 将自定义的扩展词典文件添加到 IKAnalyzer.cfg.xml 配置中

```bash
# 插件命令安装方式
vi /usr/elasticsearch/config/analysis-ik/IKAnalyzer.cfg.xml

# 安装包安装方式
vi /usr/elasticsearch/plugins/analysis-ik/config/IKAnalyzer.cfg.xml
```

- 修改内容

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
	<comment>IK Analyzer 扩展配置</comment>
 
	<!--用户可以在这里配置自己的扩展字典 -->
	<entry key="ext_dict">td_ext_dict.dic</entry>
 
  <!--用户可以在这里配置自己的扩展停止词字典-->
	<entry key="ext_stopwords">td_stop_dict.dic</entry>
 
	<!--用户可以在这里配置远程扩展字典 -->
	<!-- <entry key="remote_ext_dict">words_location</entry> -->
 
	<!--用户可以在这里配置远程扩展停止词字典-->
	<!-- <entry key="remote_ext_stopwords">words_location</entry> -->
</properties>
```



### 步骤4：重启 es



### 步骤5：测试

- 测试结果：有“江大桥”，没有“市长”

```bash
POST _analyze
{
  "analyzer": "ik_max_word",
  "text": "南京市长江大桥"
}
```



## 5、同义词词典

- 扩展词和停用词是在索引的时候使用，而同义词是检索时候使用。
- Elasticsearch 自带一个名为 synonym 的同义词 filter



### 步骤1：添加 synonym.txt

```bash
vi /usr/elasticsearch/config/analysis-ik/synonym.txt

# 添加内容
china,中国

# 重启ES
```



### 步骤2：在索引创建模板中配置同义词

- 创建索引时，使用同义词配置，模板如下

```bash
PUT /索引名称
{
  "settings": {
    "analysis": {
      "filter": {
        "word_sync": {
          "type": "synonym",
          "synonyms_path": "analysis-ik/synonym.txt"
        }
      },
      "analyzer": {
        "ik_sync_max_word": {
          "filter": [
            "word_sync"
          ],
          "type": "custom",
          "tokenizer": "ik_max_word"
        },
        "ik_sync_smart": {
          "filter": [
            "word_sync"
          ],
          "type": "custom",
          "tokenizer": "ik_smart"
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "字段名": {
        "type": "字段类型",
        "analyzer": "ik_sync_smart",
        "search_analyzer": "ik_sync_smart"
      }
    }
  }
}
```

- 示例

```bash
PUT /td-es-synonym
{
  "settings": {
    "analysis": {
      "filter": {
        "word_sync": {
          "type": "synonym",
          "synonyms_path": "analysis-ik/synonym.txt"
        }
      },
      "analyzer": {
        "ik_sync_max_word": {
          "filter": [
            "word_sync"
          ],
          "type": "custom",
          "tokenizer": "ik_max_word"
        },
        "ik_sync_smart": {
          "filter": [
            "word_sync"
          ],
          "type": "custom",
          "tokenizer": "ik_smart"
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "name": {
        "type": "text",
        "analyzer": "ik_sync_max_word",
        "search_analyzer": "ik_sync_max_word"
      }
    }
  }
}
```



### 步骤3：测试数据

```bash
# 插入数据
POST /td-es-synonym/_doc/1 
{ 
"name":"中华人民共和国，简称中国"
}

# 使用同义词“china”进行搜索
POST /td-es-synonym/_doc/_search
{
  "query": {
    "match": {
      "name": "china"
    }
  }
}
```



# 第一章 ElasticSearch 操作命令

## 1、索引操作

### 1.1 创建索引库

```bash
# 格式
# settings：就是索引库设置，其中可以定义索引库的各种属性，比如分片数、副本数等
PUT /索引名称
{
  "settings": {
    "属性名": "属性值"
  }
}

# 示例
PUT /td-company-index

# 执行后显示如下，表示成功创建
{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "td-company-index"
}
```



### 1.2 判断索引库是否存在

```bash
# 格式
HEAD /索引名称

# 示例
HEAD /td-company-index

# 表示索引库存在
200 - OK

# 表示索引库不存在
404 - Not Found
```



### 1.3 查看索引库信息

```bash
# 格式
GET /索引名称
GET /索引名称1,索引名称2,索引名称3,...

# 查看所有索引库信息
GET _all

# 查看所有索引库简化信息
# 绿色：索引的所有分片都正常分配。
# 黄色：至少有一个副本没有得到正确的分配。
# 红色：至少有一个主分片没有得到正确的分配。
GET /_cat/indices?v

# 示例
GET /td-company-index
GET /td-company-index1,td-company-index2
GET _all
GET /_cat/indices?v
```



### 1.4 打开和关闭索引库

```bash
# 打开索引库
POST /索引名称/_open

# 关闭索引库
POST /索引名称/_close

# 示例
POST /td-company-index/_open
POST /td-company-index/_close
```



### 1.5 删除索引库

```bash
# 格式
DELETE /索引名称1,索引名称2,索引名称3

# 示例
DELETE /td-company-index
```



### 1.6 零停机索引重建

- 方案1：外部数据导入方案
- 方案2：基于 scroll + bulk + 索引别名方案
- 方案3：Reindex API方案



## 2、映射操作

### （1）创建映射字段

- https://www.elastic.co/guide/en/elasticsearch/reference/7.3/mapping-params.html

```bash
# 格式1：先创建索引库，后创建映射
# type：类型，可以是text、long、short、date、integer、object等
# index：是否索引，默认为true 
# store：是否存储，默认为false
# analyzer：指定分词器
PUT /索引库名/_mapping
{
  "properties": {
    "字段名": {
      "type": "类型",
      "index": true,
      "store": true,
      "analyzer": "分词器"
    }
  }
}

# 格式2：一次性创建索引和映射
PUT /索引库名称
{
  "settings": {
    "索引库属性名": "索引库属性值"
  },
  "mappings": {
    "properties": {
      "字段名": {
        "映射属性名": "映射属性值"
      }
    }
  }
}

# 示例1：先创建索引库，后创建映射
PUT /td-company-index

PUT /td-company-index/_mapping/
{
  "properties": {
    "name": {
      "type": "text",
      "analyzer": "ik_max_word"
    },
    "job": {
      "type": "text",
      "analyzer": "ik_max_word"
    },
    "logo": {
      "type": "keyword",
      "index": "false"
    },
    "payment": {
      "type": "float"
    }
  }
}

# 示例2：一次性创建索引和映射
PUT /td-employee-index
{
  "settings": {},
  "mappings": {
    "properties": {
      "name": {
        "type": "text",
        "analyzer": "ik_max_word"
      }
    }
  }
}
```



#### 映射属性 - type

- https://www.elastic.co/guide/en/elasticsearch/reference/7.3/mapping-types.html

- String 类型

```bash
text：可分词，不可参与聚合
keyword：不可分词，数据会作为完整字段进行匹配，可以参与聚合
```

- Numerical，数值类型

```bash
基本数据类型：long、interger、short、byte、double、flfloat、half_float
浮点数的高精度类型：scaled_float（需要指定一个精度因子，比如10或100。elasticsearch会把真实值乘以这个因子后存储，取出时再原值）
```

- Date，日期类型

```bash
elasticsearch可以对日期格式化为字符串存储
建议储为毫秒值，存储为long，节省空间
```

- Array，数组类型

```bash
进行匹配时，任意一个元素满足，都认为满足
排序时，如果升序则用数组中的最小值来排序，如果降序则用数组中的最大值来排序
```

- Object：对象

```bash
# 如果存储到索引库的是对象类型，会把girl变成两个字段：girl.name和girl.age
{
  name:"Jack", 
  age:21, 
  girl:{
    name: "Rose", 
    age:21 
	} 
}
```



#### 映射属性 - index

- index影响字段的索引情况

```bash
true：字段会被索引，则可以用来进行搜索。默认值就是true，不进行任何配置，所有字段都会被索引
false：字段不会被索引，不能用来搜索
```



#### 映射属性 - store

- 是否将数据进行独立存储。

```bash
# 默认设置，原始的文本会存储在 _source 里面，默认情况下其他提取出来的字段都不是独立存储的，是从_source 里面提取出来的
store:false

# 获取独立存储的字段要比从_source中解析快得多，但是也会占用更多的空间
store:true
```



#### 映射属性 - analyzer

- 指定分词器
- 一般处理中文会选择ik分词器 ik_max_word、ik_smart



### （2）查看映射关系

```bash
# 查看单个索引映射关系
GET /索引名称/_mapping
GET /td-company-index/_mapping

# 查看所有索引映射关系
GET _mapping
GET _all/_mapping
```



### （3）修改索引映射关系

- 只能增加字段
- 修改值只能删除索引，重新建立映射

```bash
# 格式
PUT /索引库名/_mapping
{
  "properties": {
    "字段名": {
      "type": "类型",
      "index": true,
      "store": true,
      "analyzer": "分词器"
    }
  }
}
```



### （4）动态映射

- Elasticsearch在遇到文档中以前未遇到的字段，使用dynamic mapping（动态映射机制） 来确定字段的数据类型并自动把新的字段添加到类型映射。
- Elastic的动态映射机制可以进行开关控制，通过设置mappings的dynamic属性

```bash
# dynamic = true：遇到陌生字段就执行dynamic mapping处理机制
# dynamic = false：遇到陌生字段就忽略
# dynamic = strict：遇到陌生字段就报错

# 设置为报错 
PUT /user
{
  "settings": {
    "number_of_shards": 3,
    "number_of_replicas": 0
  },
  "mappings": {
    "dynamic": "strict",
    "properties": {
      "name": {
        "type": "text"
      },
      "address": {
        "type": "object",
        "dynamic": true
      }
    }
  }
}

# 插入以下文档，将会报错 
# user索引层设置dynamic是strict，在user层内设置age将报错 
# address层设置dynamic是ture，将动态映射生成字段 
PUT /user/_doc/1
{
  "name": "lisi",
  "age": "20",
  "address": {
    "province": "beijing",
    "city": "beijing"
  }
}
```



### （5）自定义动态映射 - 日期检测

- 在根对象上设置 date_detection 为 false 来关闭

```bash
# 是第一次识别 note 字段，它会被添加为 date 字段
PUT /my_index/_doc/1
{
  "note": "2014-01-01"
} 

# 这个字段已经是一个日期类型，这个不合法的日期将会造成一个异常
PUT /my_index/_doc/2
{
  "note": "Logged out"
}

#########################################################

# 关闭日期检测
PUT /my_index
{
  "mappings": {
    "date_detection": false
  }
}

# 是第一次识别 note 字段，它会被添加为 date 字段
PUT /my_index1/_doc/1
{
  "note": "2014-01-01"
} 

# 因为已经关闭日期检测，所以不会报错
PUT /my_index1/_doc/2
{
  "note": "Logged out"
} 
```



### （6）自定义动态映射 - 指定日期规则

- 指定日期规则，使用 dynamic_date_formats 设置

```bash
# 指定日期规则
PUT /my_index2
{
  "mappings": {
    "dynamic_date_formats": "MM/dd/yyyy"
  }
}

# 不符合日期规则
PUT /my_index2/_doc/1
{
  "note": "2014-01-01"
} 

# 映射类型为：text
GET /my_index2/_mapping

#######################################
# 指定日期规则
PUT /my_index3
{
  "mappings": {
    "dynamic_date_formats": "MM/dd/yyyy"
  }
}

# 符合日期规则
PUT /my_index3/_doc/1
{
  "note": "01/01/2014"
}

# 映射类型为：date
GET /my_index3/_mapping
```



### （7）自定义动态映射 - 动态模板，dynamic_templates

- 可以完全控制新生成字段的映射，甚至可以通过字段名称或数据类型来应用不同的映射
- 每个模板都有一个名称，可以用来描述这个模板的用途，一个 mapping 来指定映射应该怎样使用，以及至少一个参数 (如 match) 来定义这个模板适用于哪个字段

```bash
# es：以 _es 结尾的字段名需要使用 spanish 分词器
# en：所有其他字段使用 english 分词器
PUT /my_index4
{
  "mappings": {
    "dynamic_templates": [
      {
        "es": {
          "match": "*_es",
          "match_mapping_type": "string",
          "mapping": {
            "type": "text",
            "analyzer": "spanish"
          }
        }
      },
      {
        "en": {
          "match": "*",
          "match_mapping_type": "string",
          "mapping": {
            "type": "text",
            "analyzer": "english"
          }
        }
      }
    ]
  }
}

# 查看映射
GET /my_index4/_mapping

# 匹配字段名以 _es 结尾的字段
# 匹配其他所有字符串类型字段
PUT /my_index4/_doc/1
{
  "name_es": "testes",
  "name": "es"
}
```



## 3、文档操作

- 文档，即索引库中的数据，会根据规则创建索引，将来用于搜索。可以类比做数据库中的一行数据

### （1）新增文档

- 手动指定id

```bash
# 格式
POST /索引名称/_doc/{id}

# 举例
POST /td-company-index/_doc/1
{
  "name": "百度",
  "job": "小度用户运营经理",
  "payment": "30000",
  "logo": "http://www.lgstatic.com/thubnail_120x120/i/image/M00/21/3E/CgpFT1kVdzeAJNbU AABJB7x9sm8374.png"
}
```

- 自动生成id

```bash
# 格式
POST /索引名称/_doc { "field":"value" }

# 举例
POST /td-company-index/_doc/
{
  "name": "百度",
  "job": "小度用户运营经理",
  "payment": "30000",
  "logo": "http://www.lgstatic.com/thubnail_120x120/i/image/M00/21/3E/CgpFT1kVdzeAJNbU AABJB7x9sm8374.png"
}
```



### （2）查看文档

```bash
# 查看单个文档
GET /索引名称/_doc/{id}
GET /td-company-index/_doc/1

# 查看所有文档
POST /索引名称/_search
{
  "query": {
    "match_all": {}
  }
}

POST /td-company-index/_search
{
  "query": {
    "match_all": {}
  }
}

# _source定制返回结果
GET /td-company-index/_doc/1?_source=name,job
```

- 查看文档返回结果

```bash
# _index
document所属index

# _type 
document所属type，Elasticsearch7.x默认type为_doc

# _id
代表document的唯一标识，与index和type一起，可以唯一标识和定位一个document

# _version
document的版本号，Elasticsearch利用_version (版本号)的方式来确保应用中相互冲突的变更不会导致数据丢失。需要修改数据时，需要指定想要修改文
档的version号，如果该版本不是当前版本号，请求将会失败

# _seq_no
严格递增的顺序号，每个文档一个，Shard级别严格递增，保证后写入的Doc seq_no大于先写入的Doc seq_no

# _primary_term
任何类型的写操作，包括index、create、update和Delete，都会生成一个_seq_no。

# found 
true/false，是否查找到文档

# _source 
存储原始文档
```



### （3）更新文档

- 全部更新：是直接把之前的老数据，标记为删除状态，然后，再添加一条更新的

- id对应文档存在，则修改
- id对应文档不存在，则新增

```bash
# 格式
PUT /索引名称/_doc/{id}

# 举例
PUT /td-company-index/_doc/1
{
  "name": "百度",
  "job": "小度用户运营经理",
  "payment": "30000",
  "logo": "http://www.lgstatic.com/thubnail_120x120/i/image/M00/21/3E/CgpFT1kVdzeAJNbU AABJB7x9sm8374.png"
}
```

- 局部更新：只是修改某个字段

```bash
# 格式
POST /索引名/_update/{id}
{
  "doc": {
    "field": "value"
  }
}
```



### （4）删除文档

```bash
# 根据 id 进行删除
DELETE /索引名/_doc/{id}

# 删除所有文档
POST 索引名/_delete_by_query
{
  "query": {
    "match_all": {}
  }
}

# 根据查询条件进行删除
POST /索引库名/_delete_by_query
{
  "query": {
    "match": {
      "字段名": "搜索关键字"
    }
  }
}

# 举例
POST /td-company-index/_delete_by_query
{
  "query": {
    "match": {
      "name": "1"
    }
  }
}
```



### （5）文档的全量替换、强制创建

- 全量替换

```bash
- 语法与创建文档是一样的，如果文档id不存在，那么就是创建；如果文档id已经存在，那么就是全量替换操作，替换文档的json串内容；
- 文档是不可变的，如果要修改文档的内容，第一种方式就是全量替换，直接对文档重新建立索引，替换里面所有的内容，elasticsearch会将老的文档标记为deleted，然后新增我们给定的一个文档，当我们创建越来越多的文档的时候，elasticsearch会在适当的时机在后台自动删除标记为deleted的文档
```

- 强制创建

```bash
# 如果id 存在就会报错
PUT /index/_doc/{id}?op_type=create {}，PUT /index/_doc/{id}/_create {}
```



### （6）批量查询文档（mget）

```bash
# 不同索引
GET /_mget
{
  "docs": [
    {
      "_index": "book",
      "_id": 1
    },
    {
      "_index": "book",
      "_id": 2
    }
  ]
}

# 同一索引
GET /book/_mget
{
  "docs": [
    {
      "_id": 2
    },
    {
      "_id": 3
    }
  ]
}

# 同一索引，简化写法
POST /book/_search
{
  "query": {
    "ids": {
      "values": [
        "1",
        "4"
      ]
    }
  }
}
```



### （7）批量增删改文档（bulk）

- 实际用法：bulk请求一次不要太大，否则一下积压到内存中，性能会下降。所以，一次请求几千个操作、大小在几M正好。
- bulk会将要处理的数据载入内存中，所以数据量是有限的，最佳的数据两不是一个确定的数据，它取决于硬件、文档大小、复杂性、索引以及搜索的负载。
- 一般建议是1000-5000个文档，大小建议是5-15MB，默认不能超过100M，可以在es的配置文件（ES的 config下的elasticsearch.yml）中配置。
- http.max_content_length: 10mb

```bash
# 格式
# delete：删除一个文档，只要1个json串就可以了 删除的批量操作不需要请求体
# create：相当于强制创建 PUT /index/type/id/_create
# update：执行的是局部更新partial update操作
# index：普通的put操作，可以是创建文档，也可以是全量替换文档
POST /_bulk 
{"action":{"metadata"}}
{"data"}

# 删除1，新增5，修改2
POST /_bulk
{"delete":{"_index":"book","_id":"1"}}
{"create":{"_index":"book","_id":"5"}}
{"name":"test14","price":100.99}
{"update":{"_index":"book","_id":"2"}}
{"doc":{"name":"test"}}

```



## 4、地理坐标点操作（经纬度操作）

### （1）地理坐标点 - 创建

- 地理坐标点是指地球表面可以用经纬度描述的一个点
- 地理坐标点可以用来计算两个坐标间的距离，还可以判断一个坐标是否在一个区域中
- 地理坐标点需要显式声明对应字段类型为 geo_point 

```bash
# 格式
PUT /company-locations
{
  "mappings": {
    "properties": {
      "name": {
        "type": "text"
      },
      "location": {
        "type": "geo_point"
      }
    }
  }
}

# 字符串形式
PUT /company-locations/_doc/1
{
  "name": "NetEase",
  "location": "40.715,74.011"
}

# 对象形式 
PUT /company-locations/_doc/2
{
  "name": "Sina",
  "location": {
    "lat": 40.722,
    "lon": 73.989
  }
}

# 数组形式 
PUT /company-locations/_doc/3
{
  "name": "Baidu",
  "location": [
    73.983,
    40.719
  ]
}
```



### （2）地理坐标点 - 查询 

- geo_bounding_box：找出落在指定矩形框中的点（指定一个矩形的顶部、底部、左边界、右边界，然后过滤器只需判断坐标的经度是否在左右边界之间，纬度是否在上下边界之间）
- geo_distance：找出与指定位置在给定距离内的点
- geo_distance_range：找出与指定点距离在给定最小距离和最大距离之间的点
- geo_polygon：找出落在多边形中的点。 这个过滤器使用代价很大 。使用前先看看 geo-shapes

```bash
# 使用 geo_bounding_box 过滤器查询
GET /company-locations/_search
{
  "query": {
    "bool": {
      "must": {
        "match_all": {}
      },
      "filter": {
        "geo_bounding_box": {
          "location": {
            "top_left": {
              "lat": 40.73,
              "lon": 71.12
            },
            "bottom_right": {
              "lat": 40.01,
              "lon": 74.1
            }
          }
        }
      }
    }
  }
}

# 使用 geo_distance 过滤器查询
GET /company-locations/_search
{
  "query": {
    "bool": {
      "must": {
        "match_all": {}
      },
      "filter": {
        "geo_distance": {
          "distance": "200km",
          "location": {
            "lat": 40,
            "lon": 70
          }
        }
      }
    }
  }
}
```



## 5、查询操作 - Query DSL

```bash
# 格式
POST /索引库名/_search
{
  "query": {
    "查询类型": {
      "查询条件": "查询条件值"
    }
  }
}
```



### （1）查询所有（match_all）

```bash
# query：代表查询对象
# match_all：代表查询所有
POST /td-company-index/_search
{
  "query": {
    "match_all": {}
  }
}

# 查询结果分析
# took：查询花费时间，单位是毫秒
# time_out：是否超时
# _shards：分片信息
# hits：搜索结果总览对象
#       total：搜索到的总条数
#       max_score：所有结果中文档得分的最高分
#       hits：搜索结果的文档对象数组，每个元素是一条搜索到的文档信息
#             _index：索引库
#             _type：文档类型
#             _id：文档id
#             _score：文档得分
#             _source：文档的源数据
```



### （2）全文搜索（full-text query）

#### 匹配搜索（match query）

- 可以对一个字段进行模糊、短语查询
- match queries 接收 text/numerics/dates，对它们进行分词分析，再组织成一个boolean查询
- 可通过operator 指定bool组合操作（or、and 默认是 or ）

- 数据准备

```bash
PUT /td-property
{
  "settings": {},
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "ik_max_word"
      },
      "images": {
        "type": "keyword"
      },
      "price": {
        "type": "float"
      }
    }
  }
}

# 数据
POST /td-property/_doc/
{
  "title": "小米电视4A",
  "images": "http://image.com/12479122.jpg",
  "price": 4288
}

POST /td-property/_doc/
{
  "title": "小米手机",
  "images": "http://image.com/12479622.jpg",
  "price": 2699
}

POST /td-property/_doc/
{
  "title": "华为手机",
  "images": "http://image.com/12479922.jpg",
  "price": 5699
}
```

- 查询

```bash
# or 关系查询（默认），会把查询条件进行分词，然后进行查询
POST /td-property/_search
{
  "query": {
    "match": {
      "title": "小米电视4A"
    }
  }
}

# and 关系查询
POST /td-property/_search
{
  "query": {
    "match": {
      "title": {
        "query": "小米电视4A",
        "operator": "and"
      }
    }
  }
}
```



#### 短语搜索（match phrase query）

- 用来对一个字段进行短语查询，可以指定 analyzer、slop移动因子

```bash
# 查询结果为0，因为"小米 4A"，中间有其他词
GET /td-property/_search
{
  "query": {
    "match_phrase": {
      "title": "小米 4A"
    }
  }
}

# 有查询结果，因为"slop"移动因子设置为2，表示跨度为2
GET /td-property/_search
{
  "query": {
    "match_phrase": {
      "title": {
        "query": "小米 4A",
        "slop": 2
      }
    }
  }
}
```



#### query_string 查询

- 无需指定某字段而对文档全文进行匹配查询的一个高级查询，同时可以指定在 哪些字段上进行匹配

```bash
# 查询所有字段中，值为2699
GET /td-property/_search
{
  "query": {
    "query_string": {
      "query": "2699"
    }
  }
}

# 指定查询 title 字段中，值为2699
GET /td-property/_search
{
  "query": {
    "query_string": {
      "query": "2699",
      "default_field": "title"
    }
  }
}

# 逻辑查询，查询 title 字段中，值包括"手机" OR "小米"
GET /td-property/_search
{
  "query": {
    "query_string": {
      "query": "手机 OR 小米",
      "default_field": "title"
    }
  }
}

# 逻辑查询，查询 title 字段中，值包括"手机" and "小米"
GET /td-property/_search
{
  "query": {
    "query_string": {
      "query": "手机 AND 小米",
      "default_field": "title"
    }
  }
}

# 模糊查询，查询title中"大","米","大米"（大米，允许忽略其中一个字）
GET /td-property/_search
{
  "query": {
    "query_string": {
      "query": "大米~1",
      "default_field": "title"
    }
  }
}

# 多字段支持， title、price字段中有一个值为2699的
GET /td-property/_search
{
  "query": {
    "query_string": {
      "query": "2699",
      "fields": [
        "title",
        "price"
      ]
    }
  }
}
```



#### 多字段匹配搜索（multi match query）

```bash
GET /td-property/_search
{
  "query": {
    "multi_match": {
      "query": "2699",
      "fields": [
        "title",
        "price"
      ]
    }
  }
}


# *匹配多个字段
GET /td-property/_search
{
  "query": {
    "multi_match": {
      "query": "http://image.com/12479622.jpg",
      "fields": [
        "title",
        "ima*"
      ]
    }
  }
}
```



### （3）词条级搜索（term-level queries）

- 数据准备

```bash
	PUT /book
{
  "settings": {},
  "mappings": {
    "properties": {
      "description": {
        "type": "text",
        "analyzer": "ik_max_word"
      },
      "name": {
        "type": "text",
        "analyzer": "ik_max_word"
      },
      "price": {
        "type": "float"
      },
      "timestamp": {
        "type": "date",
        "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
      }
    }
  }
}


PUT /book/_doc/1
{
  "name": "lucene",
  "description": "Lucene Core is a Java library providing powerful indexing and search features, as well as spellchecking, hit highlighting and advanced analysis/tokenization capabilities. The PyLucene sub project provides Python bindings for Lucene Core. ",
  "price": 100.45,
  "timestamp": "2020-08-21 19:11:35"
}

PUT /book/_doc/2
{
  "name": "solr",
  "description": "Solr is highly scalable, providing fully fault tolerant distributed indexing, search and analytics. It exposes Lucenes features through easy to use JSON/HTTP interfaces or native clients for Java and other languages.",
  "price": 320.45,
  "timestamp": "2020-07-21 17:11:35"
}

PUT /book/_doc/3
{
  "name": "Hadoop",
  "description": "The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.",
  "price": 620.45,
  "timestamp": "2020-08-22 19:18:35"
}

PUT /book/_doc/4
{
  "name": "ElasticSearch",
  "description": "Elasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力 的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条 款下的开放源码发布，是一种流行的企业级搜索引擎。Elasticsearch用于云计算中，能够达到实时搜 索，稳定，可靠，快速，安装使用方便。官方客户端在Java、.NET（C#）、PHP、Python、Apache Groovy、Ruby和许多其他语言中都是可用的。根据DB-Engines的排名显示，Elasticsearch是最受欢 迎的企业搜索引擎，其次是Apache Solr，也是基于Lucene。",
  "price": 999.99,
  "timestamp": "2020-08-15 10:11:35"
}
```

- 搜素

```bash
# 词条搜索（term query）
# term 查询用于查询指定字段包含某个词项的文档
POST /book/_search
{
  "query": {
    "term": {
      "name": "solr"
    }
  }
}

# 词条集合搜索（terms query）
# terms 查询用于查询指定字段包含某些词项的文档
GET /book/_search
{
  "query": {
    "terms": {
      "name": [
        "solr",
        "elasticsearch"
      ]
    }
  }
}

# 范围搜索（range query）
# gte：大于等于
# gt：大于
# lte：小于等于
# lt：小于
# boost：查询权重
GET /book/_search
{
  "query": {
    "range": {
      "price": {
        "gte": 10,
        "lte": 200,
        "boost": 2
      }
    }
  }
}

GET /book/_search
{
  "query": {
    "range": {
      "timestamp": {
        "gte": "now-2d/d",
        "lt": "now/d"
      }
    }
  }
}

GET book/_search
{
  "query": {
    "range": {
      "timestamp": {
        "gte": "18/08/2020",
        "lte": "2021",
        "format": "dd/MM/yyyy||yyyy"
      }
    }
  }
}

# 不为空搜索（exists query）
# 查询指定字段值不为空的文档，相当 SQL 中的 column is not null
GET /book/_search
{
  "query": {
    "exists": {
      "field": "price"
    }
  }
}

# 词项前缀搜索（prefix query）
GET /book/_search
{
  "query": {
    "prefix": {
      "name": "so"
    }
  }
}

# 通配符搜索（wildcard query）
# 区分大小写
GET /book/_search
{
  "query": {
    "wildcard": {
      "name": "so*r"
    }
  }
}

GET /book/_search
{
  "query": {
    "wildcard": {
      "name": {
        "value": "lu*",
        "boost": 2
      }
    }
  }
}

# 正则搜索（regexp query）
# 如.*开头的查询，将会匹配所有的倒排索引中的关键字，这几乎相当于全表扫描，会很慢。因此如果可以的话，最好在使用正则前，加上匹配的前缀
GET /book/_search
{
  "query": {
    "regexp": {
      "name": "s.*"
    }
  }
}

GET /book/_search
{
  "query": {
    "regexp": {
      "name": {
        "value": "s.*",
        "boost": 1.2
      }
    }
  }
}

# 模糊搜索（fuzzy query）
GET /book/_search
{
  "query": {
    "fuzzy": {
      "name": "so"
    }
  }
}

# fuzziness，模糊值，允许模糊几个
GET /book/_search
{
  "query": {
    "fuzzy": {
      "name": {
        "value": "so",
        "boost": 1,
        "fuzziness": 2
      }
    }
  }
}

GET /book/_search
{
  "query": {
    "fuzzy": {
      "name": {
        "value": "sorl",
        "boost": 1,
        "fuzziness": 2
      }
    }
  }
}

# ids搜索（id集合查询）
GET /book/_search
{
  "query": {
    "ids": {
      "type": "_doc",
      "values": [
        "1",
        "3"
      ]
    }
  }
}
```



### （4）复合搜索（compound query）

```bash
# constant_score query
# 固定分数查询：包装另一个查询，将查询匹配的文档的评，设为一个常值
GET /book/_search
{
  "query": {
    "constant_score": {
      "filter": {
        "term": {
          "description": "solr"
        }
      },
      "boost": 1.2
    }
  }
}


# 布尔搜索（bool query）
# 组合多个查询字句为一个查询
# bool must：必须满足
# bool filter：必须满足，但执行的是filter上下文，不参与、不影响评分
# bool should：或
# bool must_not：必须不满足，在filter上下文中执行，不参与、不影响评分
# minimum_should_match：代表了最小匹配精度，如果设置minimum_should_match=1，那么should语句中至少需要有一个条件满足
POST /book/_search
{ 
  "query": {
    "bool": {
      "should": {
        "match": {
          "description": "java"
        }
      },
      "filter": {
        "term": {
          "name": "solr"
        }
      },
      "must_not": {
        "range": {
          "price": {
            "gte": 200,
            "lte": 300
          }
        }
      },
      "minimum_should_match": 1,
      "boost": 1
    }
  }
}
```



### （5）排序

```bash
# 评分升序排序
POST /book/_search
{
  "query": {
    "match": {
      "description": "solr"
    }
  },
  "sort": [
    {
      "_score": {
        "order": "asc"
      }
    }
  ]
}

# 字段值排序，降序
POST /book/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "price": {
        "order": "desc"
      }
    }
  ]
}

# 多级排序
# 首先按照价格排序，然后按照相关性得分排序
POST /book/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "price": {
        "order": "desc"
      }
    },
    {
      "timestamp": {
        "order": "desc"
      }
    }
  ]
}
```



### （6）分页

```bash
# size：每页显示多少条
# from：当前页起始索引, int start = (pageNum - 1) * size
POST /book/_search
{
  "query": {
    "match_all": {}
  },
  "size": 2,
  "from": 0
}

POST /book/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "price": {
        "order": "desc"
      }
    }
  ],
  "size": 2,
  "from": 2
}
```



### （7）高亮

```bash
# pre_tags：前置标签
# post_tags：后置标签
# fields：需要高亮的字段

# 单字段
POST /book/_search
{
  "query": {
    "match": {
      "name": "elasticsearch"
    }
  },
  "highlight": {
    "pre_tags": "<font color='pink'>",
    "post_tags": "</font>",
    "fields": [
      {
        "name": {}
      }
    ]
  }
}

# 单字段高亮：query match 已经限制查询name了
POST /book/_search
{
  "query": {
    "match": {
      "name": "elasticsearch"
    }
  },
  "highlight": {
    "pre_tags": "<font color='pink'>",
    "post_tags": "</font>",
    "fields": [
      {
        "name": {}
      },
      {
        "description": {}
      }
    ]
  }
}

# 多字段高亮
POST /book/_search
{
  "query": {
    "query_string": {
      "query": "elasticsearch"
    }
  },
  "highlight": {
    "pre_tags": "<font color='pink'>",
    "post_tags": "</font>",
    "fields": [
      {
        "name": {}
      },
      {
        "description": {}
      }
    ]
  }
}
```



## 6、查询操作 - Filter DSL

- Elasticsearch中的所有的查询都会触发相关度得分的计算
- Filter DSL 执行速度快

```
过滤器不会计算相关度的得分，所以它们在计算上更快一些。
过滤器可以被缓存到内存中，这使得在重复的搜索查询上，其要比相应的查询快出许多
```

- 写法

```bash
# ES 5.0 之后的写法 
# 被过滤的查询包含一个match_all查询（查询部分）和一个过滤器（filter部分）
POST /book/_search
{
  "query": {
    "bool": {
      "must": {
        "match_all": {}
      },
      "filter": {
        "range": {
          "price": {
            "gte": 200,
            "lte": 1000
          }
        }
      }
    }
  }
}
```



## 7、聚合分析

### 7.1 聚合分析

```bash
# 格式（aggregations 也可简写为 aggs）
"aggregations" : 
{ "<aggregation_name>" : 
  {
    "<aggregation_type>" : { <aggregation_body> }
    [,"meta" : { [<meta_data_body>] } ]? 
    [,"aggregations" : { [<sub_aggregation>]+ } ]?
  }

  [,"<aggregation_name_2>" : { ... } ]
}

# 解释
"aggregations" : 
{ "聚合名称1" : 
  {
    "聚合类型" : { 聚合体（对哪些字段聚合） }
    [,"meta" : { [元数据] } ]? 
    [,"aggregations" : { [子聚合]+ } ]?
  }

  [,"聚合名称2" : { ... } ]
}
```



### 7.2 指标聚合 metric

- 指标聚合 metric：对一个数据集求最大、最小、和、平均值等指标的聚合

```bash
# max min sum avg
POST /book/_search
{
  "size": 0,
  "aggs": {
    "max_price": {
      "max": {
        "field": "price"
      }
    }
  }
}

# count：文档计数
# 统计price大于100的文档数量
POST /book/_count
{
  "query": {
    "range": {
      "price": {
        "gt": 100
      }
    }
  }
}

# value_count：统计某字段有值的文档数
POST /book/_search?size=0
{
  "aggs": {
    "price_count": {
      "value_count": {
        "field": "price"
      }
    }
  }
}

POST /book/_search
{
  "size": 0,
  "aggs": {
    "price_count": {
      "value_count": {
        "field": "price"
      }
    }
  }
}

# cardinality：去重
POST /book/_search?size=0
{
  "aggs": {
    "_id_count": {
      "cardinality": {
        "field": "_id"
      }
    },
    "price_count": {
      "cardinality": {
        "field": "price"
      }
    }
  }
}

# stats：统计 count max min avg sum 5个值的状态
POST /book/_search?size=0
{
  "aggs": {
    "price_stats": {
      "stats": {
        "field": "price"
      }
    }
  }
}

# Extended stats：统计 count max min avg sum 平方和、方差、标准差、平均值加/减两个标准差的区间
POST /book/_search?size=0
{
  "aggs": {
    "price_stats": {
      "extended_stats": {
        "field": "price"
      }
    }
  }
}

# Percentiles：占比百分位对应的值统计
POST /book/_search?size=0
{
  "aggs": {
    "price_percents": {
      "percentiles": {
        "field": "price"
      }
    }
  }
}

# 指定显示哪些百分比
POST /book/_search?size=0
{
  "aggs": {
    "price_percents": {
      "percentiles": {
        "field": "price",
        "percents": [
          75,
          99,
          99.9
        ]
      }
    }
  }
}

# Percentiles rank 统计值小于等于指定值的文档占比
# 统计price小于100和200的文档的占比
POST /book/_search?size=0
{
  "aggs": {
    "gge_perc_rank": {
      "percentile_ranks": {
        "field": "price",
        "values": [
          100,
          200
        ]
      }
    }
  }
}
```



### 7.3 桶聚合 buketing

- 桶聚合 bucketing：对查询出的数据进行分组 group by（分桶），再在组上进行指标聚合，桶聚合 bucketing
- https://www.elastic.co/guide/en/elasticsearch/reference/7.3/search-aggregations-bucket.html

- 使用

```bash
# 分组后，再统计
POST /book/_search
{
  "size": 0,
  "aggs": {
    "group_by_price": {
      "range": {
        "field": "price",
        "ranges": [
          {
            "from": 0,
            "to": 200
          },
          {
            "from": 200,
            "to": 400
          },
          {
            "from": 400,
            "to": 1000
          }
        ]
      },
      "aggs": {
        "average_price": {
          "avg": {
            "field": "price"
          }
        },
        "count_price": {
          "value_count": {
            "field": "price"
          }
        }
      }
    }
  }
}

# 实现 having 过滤
POST /book/_search
{
  "size": 0,
  "aggs": {
    "group_by_price": {
      "range": {
        "field": "price",
        "ranges": [
          {
            "from": 0,
            "to": 200
          },
          {
            "from": 200,
            "to": 400
          },
          {
            "from": 400,
            "to": 1000
          }
        ]
      },
      "aggs": {
        "average_price": {
          "avg": {
            "field": "price"
          }
        },
        "having": {
          "bucket_selector": {
            "buckets_path": {
              "avg_price": "average_price"
            },
            "script": {
              "source": "params.avg_price >= 200 "
            }
          }
        }
      }
    }
  }
}
```



## 8、智能搜索建议 Suggester

- Term Suggester
- Phrase Suggester
- Completion Suggester
- Context Suggeste



## 9、定位非法搜索及原因

```bash
# 使用 validate + explain 定位非法搜索及原因
GET /book/_validate/query?explain
{
  "query": {
    "match1": {
      "name": "test"
    }
  }
}
```



# 第二章 ElasticSearch 简介

## 1、概述

- Elaticsearch 简称为 ES，是一个开源的可扩展的分布式的全文检索引擎，它可以近乎实时的存储、检索数据。
- 本身扩展性很好，可扩展到上百台服务器，处理 PB 级别的数据。
- ES 使用 Java 开发并使用 Lucene 作为其核心来实现索引和搜索的功能
- 通过简单的 RestfulAPI 和 javaAPI 来隐藏 Lucene 的复杂性，从而让全文搜索变得简单。



## 2、特点

- 提供了一个极速的搜索体验。这源于它的高速（speed）。相比较其它的一些大数据引擎，Elasticsearch可以实现秒级的搜索，速度非常有优势。Elasticsearch的cluster是一种分布式的部署，极易扩展(scale )这样很容易使它处理PB级的数据库容量。最重要的是Elasticsearch是它搜索的结果可以按照分数进行排序，它能提供我们最相关的搜索结果（relevance) 。
- 安装方便：没有其他依赖，下载后安装非常方便；只用修改几个参数就可以搭建起来一个集群
- JSON：输入/输出格式为 JSON，意味着不需要定义 Schema，快捷方便
- RESTful：基本所有操作 ( 索引、查询、甚至是配置 ) 都可以通过 HTTP 接口进行
- 分布式：节点对外表现对等（每个节点都可以用来做入口） 加入节点自动负载均衡
- 多租户：可根据不同的用途分索引，可以同时操作多个索引
- 支持超大数据： 可以扩展到 PB 级的结构化和非结构化数据 海量数据的近实时处理



## 3、版本

- Elasticsearch 主流版本为5.x、6.x、7.x版本



### 3.1 Elasticsearch 7.x 更新的内容

#### （1）集群连接变化：TransportClient被废弃

- es7的java代码，只能使用restclient。对于java编程，建议采用 High-level-rest- client 的方式操作ES集群
- High-level REST client 已删除接受Header参数的API方 法，Cluster Health API默认为集群级别。



#### （2）ES数据存储结构变化：简化了Type 默认使用_doc

- es6时，官方就提到了es7会逐渐删除索引type，并且es6时已经规定每一个index只能有一个 type
- 在es7中使用默认的_doc作为type，官方说在8.x版本会彻底移除type
- api请求方式也发送变化，如获得某索引的某ID的文档：GET index/_doc/id其中index和id为具体的值



#### （3）ES程序包默认打包jdk

- 7.x版本的程序包大小突然增大了200MB+， 对比6.x发现，包大了200MB+， 正是JDK的大小



#### （4）默认配置变化

- 默认节点名称为主机名，默认分片数改为1，不再是5。 



#### （5）升级到 lucene 8 查询相关性速度优化

- Weak-AND算法
- es可以看成是分布式lucene，lucene的性能直接决定es的性能。
- lucene8在top k及其他查询上有很大的性能提升。
- weak-and算 核心原理：取TOP N结果集，估算命中记录数。 TOP N的时候会跳过得分低于10000的文档来达到更快的性能。



#### （6）间隔查询(Intervals queries)

- intervals query 允许用户精确控制查询词在文档中出现的先后关系，实现了对terms顺序、terms之间的距离以及它们之间的包含关系的灵活控制。



#### （7）引入新的集群协调子系统

- 移除 minimum_master_nodes 参数，让 Elasticsearch 自己选择可以形成仲裁的节点。



#### （8）7.0 将不会再有OOM的情况

- JVM引入了新的 circuit breaker（熔断）机制，当查询或聚合的数据量超出单机处理的最大内存限制时会被截断。
- 设置 indices.breaker.fielddata.limit 的默认值已从JVM堆大小的60％降低到40％。



#### （9）分片搜索空闲时跳过 refresh

- 以前版本的数据插入，每一秒都会有refresh动作，这使得es能成为一个近实时的搜索引擎。
- 当没有查询需求的时候，该动作会使得es的资源得到较大的浪费。



### 3.2 兼容性 - Elasticsearch与操作系统

- https://www.elastic.co/cn/support/matrix#matrix_os

![image-20211228172717394](image/image-20211228172717394.png)



### 3.3 兼容性 - Elasticsearch and JVM

- https://www.elastic.co/cn/support/matrix#matrix_jvm

![image-20211228172839959](image/image-20211228172839959.png)



## 4、使用场景

### （1）分布式的搜索引擎

- 分布式：Elasticsearch自动将海量数据分散到多台服务器上去存储和检索

- 全文检索：提供模糊搜索等自动度很高的查询方式，并进行相关性排名，高亮等功能

- 搜索：百度、谷歌，站内搜索、电商网站、招聘网站、新闻资讯类网站、各种app内的搜索

  - 维基百科、百度百科：有全文检索、高亮、搜索推荐功能
  - stack overflow：有全文检索，可以根据报错关键信息，去搜索解决方法。
  - github：从上千亿行代码中搜索你想要的关键代码和项目。

  

### （2）日志分析类场景

- 经典的ELK组合（Elasticsearch/Logstash/Kibana），可以完成日志收集，日志存储，日志分析查询界面基本功能，目前该方案的实现很普及，大部分企业日志分析系统使用了该方案。

  

### （3）数据预警平台及数据分析场景（分组聚合）

- 例如电商价格预警，在支持的电商平台设置价格预警，当优惠的价格低于某个值时，触发通知消息，通知用户购买。
- 数据分析：常见的比如分析电商平台销售量top 10的品牌，分析博客系统、头条网站top 10关注度、评论数、访问量的内容等等。



### （4）对海量数据进行近实时的处理

- 海量数据的处理：因为是分布式架构，Elasticsearch可以采用大量的服务器去存储和检索数据，自然而然就可以实现海量数据的处理
- 近实时：Elasticsearch可以实现秒级别的数据搜索和分析
- 商业BI（Business Intelligence）系统：比如大型零售超市，需要分析上一季度用户消费金额，年龄段，每天各时间段到店人数分布等信息，输出相应的报表数据，并预测下一季度的热卖商品，根据年龄段定向推荐适宜产品。Elasticsearch执行数据分析和挖掘，Kibana做数据可视化。



## 5、全文搜索方案对比

- Solr和Elasticsearch都是基于Lucene实现的
- Solr和Elasticsearch之间的区别

```
Solr利用Zookpper进行分布式管理
Elasticsearch自身带有分布式协调管理功能

Solr比Elasticsearch实现更加全面
Solr官方提供的功能更多
Elasticsearch本身更注重于核心功能， 高级功能多由第三方插件提供

Solr在传统的搜索应用中表现好于Elasticsearch，
Elasticsearch在实时搜索应用方面比Solr表现好
```



### 5.1 基于Lucene

#### （1）【Lucene】全文搜索引擎框架

- 官方网站：http://lucene.apache.org/
- Lucene是Apache基金会维护的一套完全使用Java编写的信息搜索工具包（Jar包），它包含了索引结构、读写索引工具、相关性工具、排序等功能



#### （2）【ElasticSearch】分布式搜索引擎（基于Lucene框架，基于JSON进行索引）

- 官方网站：http://www.elasticsearch.org/
- Elasticsearch也是一个建立在全文搜索引擎 Apache Lucene基础上的搜索引擎
- 采用的策略是分布式实时文件存储，并将每一个字段都编入索引，使其可以被搜索

 

#### （3）【Compass】（基于Lucene框架）

- 官方网站：http://www.compass-project.org/
- 事务的，高性能的对象/搜索引擎映射（OSEM:object/search engine mapping）与一个Java持久层框架



#### （4）【LIRE】图片搜索框架（基于Lucene、基于Java）

- 官方网站：http://www.semanticmetadata.net/lire/

 

### 5.2 基于Solr

#### （1）【Solr】Java全文搜索服务器（基于Lucene框架）

- 官方网站：http://lucene.apache.org/solr/
- Solr是一个有HTTP接口的基于Lucene的查询服务器，是一个搜索引擎系统，封装了很多Lucene细节
- Solr可以直接利用HTTP GET/POST请求去查询，维护修改索引



#### （2）【Solandra】实时分布式搜索引擎（基于 Apache Solr 和 Apache Cassandra 构建）

- 官方网站：https://github.com/tjake/Solandra

 

### 5.3 基于Java

#### （1）【Nutch】开源Java搜索引擎

- 官方网站：http://nutch.apache.org/

 

#### （2）【Egothor】全文本搜索引擎（基于Java）

- 官方网站：http://www.egothor.org/cms/

 

#### （3）【IndexTank】基于Java的索引-实时全文搜索引擎实现

- 官方网站：https://github.com/linkedin/indextank-engine



# 第三章 ElasticSearch 用法

## 1、核心概念

### 1.1 索引（index）

- 类似的数据放在一个索引，非类似的数据放不同索引， 一个索引也可以理解成一个关系型数据库。



### 1.2 类型（type）

- 代表document属于index中的哪个类别（type）也有一种说法一种type就像是数据库的表，
- ES 5.x中一个index可以有多种type
- ES 6.x中一个index只能有一种type
- ES 7.x以后，要逐渐移除type这个概念



### 1.3 映射（mapping）

- mapping定义了每个字段的类型等信息。相当于关系型数据库中的表结构。
- 常用数据类型：text、keyword、number、array、range、boolean、date、geo_point、ip、nested、object



## 2、关系型数据库 VS Elasticsearch

| 关系型数据库（比如Mysql） | 非关系型数据库（Elasticsearch） |
| ------------------------- | ------------------------------- |
| 数据库 Database           | 索引 Index                      |
| 表 Table                  | 索引 Index 类型（原为Type）     |
| 数据行 Row                | 文档 Document                   |
| 数据列 Column             | 字段 Field                      |
| 约束 Schema               | 映射 Mapping                    |



## 3、Elasticsearch API

- https://www.elastic.co/guide/en/elasticsearch/client/index.html



## 4、Elasticsearch Java Client

### 步骤1：pom.xml

```xml
<dependencies>
  <dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-test</artifactId>
  </dependency>

  <dependency>
    <groupId>org.elasticsearch.client</groupId>
    <artifactId>elasticsearch-rest-high-level-client</artifactId>
    <version>7.3.0</version>
    <exclusions>
      <exclusion>
        <groupId>org.elasticsearch</groupId>
        <artifactId>elasticsearch</artifactId>
      </exclusion>
    </exclusions>
  </dependency>

  <dependency>
    <groupId>org.elasticsearch</groupId>
    <artifactId>elasticsearch</artifactId>
    <version>7.3.0</version>
  </dependency>
</dependencies>
```



### 步骤2：application.yml

```yaml
td:
  elasticsearch:
    hostlist: 192.168.203.133:9200
```



### 步骤3：配置类 ElasticsearchConfig

```java
@Configuration
public class ElasticsearchConfig {
    @Value("${td.elasticsearch.hostlist}")
    private String hostlist;

    @Bean
    public RestHighLevelClient restHighLevelClient() {
        // 解析 hostlist 信息
        String[] split = hostlist.split(",");

        // 创建HttpHost数组，封装es的主机和端口
        HttpHost[] httpHosts = new HttpHost[split.length];
        for (int i = 0; i < split.length; i++) {
            String iterm = split[i];
            httpHosts[i] = new HttpHost(iterm.split(":")[0], Integer.parseInt(iterm.split(":")[1]), "http");
        }

        return new RestHighLevelClient(RestClient.builder(httpHosts));
    }
}
```



### 步骤4：测试类

#### （1）Index

```java
@SpringBootTest
@RunWith(SpringRunner.class)
public class a_Index {
    @Autowired
    RestHighLevelClient client;

    // 创建索引库
    /*
    PUT /elasticsearch_test
    {
      "settings": {},
      "mappings": {
        "properties": {
          "description": {
            "type": "text",
            "analyzer": "ik_max_word"
          },
          "name": {
            "type": "keyword"
          },
          "pic": {
            "type": "text",
            "index": false
          },
          "studymodel": {
            "type": "keyword"
          }
        }
      }
    }
    */
    @Test
    public void testCreateIndex() throws IOException {
        // 创建一个索引创建请求对象
        CreateIndexRequest createIndexRequest = new CreateIndexRequest("elasticsearch_test");

        // 设置映射，方式1
        //XContentBuilder builder = XContentFactory.jsonBuilder()
        //        .startObject()
        //        .field("properties")
        //        .startObject()
        //        .field("description").startObject().field("type", "text").field("analyzer", "ik_max_word").endObject()
        //        .field("name").startObject().field("type", "keyword").endObject()
        //        .field("pic").startObject().field("type", "text").field("index", "false").endObject()
        //        .field("studymodel").startObject().field("type", "keyword").endObject()
        //        .endObject()
        //        .endObject();
        //createIndexRequest.mapping("doc", builder);

        // 设置映射，方式2
        createIndexRequest.mapping("doc", "{\n" +
                "        \"properties\": {\n" +
                "          \"description\": {\n" +
                "            \"type\": \"text\",\n" +
                "            \"analyzer\": \"ik_max_word\"\n" +
                "          },\n" +
                "          \"name\": {\n" +
                "            \"type\": \"keyword\"\n" +
                "          },\n" +
                "          \"pic\": {\n" +
                "            \"type\": \"text\",\n" +
                "            \"index\": false\n" +
                "          },\n" +
                "          \"studymodel\": {\n" +
                "            \"type\": \"keyword\"\n" +
                "          }\n" +
                "        }\n" +
                "      }", XContentType.JSON);

        // 操作索引的客户端
        IndicesClient indicesClient = client.indices();
        CreateIndexResponse createIndexResponse = indicesClient.create(createIndexRequest, RequestOptions.DEFAULT);

        // 得到响应
        boolean acknowledged = createIndexResponse.isAcknowledged();
        System.out.println(acknowledged);
    }

    @Test
    public void testDeleteIndex() throws IOException {
        // 构建 删除索引库的请求对象
        DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest("elasticsearch_test");
        IndicesClient indicesClient = client.indices();

        AcknowledgedResponse deleteResponse = indicesClient.delete(deleteIndexRequest, RequestOptions.DEFAULT);

        // 得到响应
        boolean acknowledge = deleteResponse.isAcknowledged();
        System.out.println(acknowledge);
    }
}
```



#### （2）doc

```java
@SpringBootTest
@RunWith(SpringRunner.class)
public class b_doc {
    @Autowired
    RestHighLevelClient client;

    //添加文档
    /*
    POST /elasticsearch_test/_doc/1
    {
      "name": "spring cloud实战",
      "description": "本课程主要从四个章节进行讲解： 1.微服务架构入门 2.spring cloud 基础入门 3.实战Spring Boot 4.注册中心eureka。",
      "studymodel":"201001",
      "timestamp": "2020-08-22 20:09:18",
      "price": 5.6
    }
     */
    @Test
    public void testAddDoc() throws IOException {
        // 准备索取请求对象
        //IndexRequest indexRequest  = new IndexRequest("elasticsearch_test","doc");
        IndexRequest indexRequest = new IndexRequest("elasticsearch_test");
        indexRequest.id("1");

        // 文档内容  准备json数据
        Map<String, Object> jsonMap = new HashMap<>();
        jsonMap.put("name", "spring cloud实战3");
        jsonMap.put("description", "本课程主要从四个章节进行讲解3： 1.微服务架构入门 2.spring cloud 基础入门 3.实战Spring Boot 4.注册中心eureka。");
        jsonMap.put("studymodel", "3101001");
        jsonMap.put("timestamp", "2020-07-22 20:09:18");
        jsonMap.put("price", 35.6);
        indexRequest.source(jsonMap);

        // 执行请求
        IndexResponse indexResponse = client.index(indexRequest, RequestOptions.DEFAULT);
        DocWriteResponse.Result result = indexResponse.getResult();
        System.out.println(result);
    }

    // 查询文档
    @Test
    public void testGetDoc() throws IOException {
        // 查询请求对象
        GetRequest getRequest = new GetRequest("elasticsearch_test", "1");
        GetResponse getResponse = client.get(getRequest, RequestOptions.DEFAULT);

        // 得到文档内容
        Map<String, Object> sourceMap = getResponse.getSourceAsMap();
        System.out.println(sourceMap);
    }
}
```



#### （3）查询

```java
@SpringBootTest
@RunWith(SpringRunner.class)
public class c_query {
    @Autowired
    RestHighLevelClient client;

    //搜索全部记录
    /*
       GET   /elasticsearch_test/_search
        {
          "query":{
             "match_all":{}
          }
        }
    */
    @Test
    public void testSearchAll() throws IOException {
        // 搜索请求对象
        SearchRequest searchRequest = new SearchRequest("elasticsearch_test");
        searchRequest.searchType(SearchType.QUERY_THEN_FETCH);

        // 搜索源构建对象
        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();

        // 设置搜索方法
        searchSourceBuilder.query(QueryBuilders.matchAllQuery());
        searchSourceBuilder.fetchSource(new String[]{"name", "price", "timestamp"}, new String[]{});

        // 请求对象设置 搜索源对象
        searchRequest.source(searchSourceBuilder);

        // 使用client  执行搜索
        SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);

        // 搜索结果
        SearchHits hits = searchResponse.getHits();

        // 匹配到的总记录数
        TotalHits totalHits = hits.getTotalHits();
        System.out.println("查询到的总记录数:" + totalHits.value);

        // 得到的匹配度高的文档
        SearchHit[] searchHits = hits.getHits();
        for (SearchHit hit : searchHits) {
            String id = hit.getId();

            // 源文档的内容
            Map<String, Object> sourceMap = hit.getSourceAsMap();
            String name = (String) sourceMap.get("name");
            String timestamp = (String) sourceMap.get("timestamp");
            String description = (String) sourceMap.get("description");
            Double price = (Double) sourceMap.get("price");
            System.out.println(name);
            System.out.println(timestamp);
            System.out.println(description);
            System.out.println(price);
        }
    }

    @Test
    public void testTermQuery() throws IOException {
        // 搜索请求对象
        SearchRequest searchRequest = new SearchRequest("elasticsearch_test");

        // 搜索源构建对象
        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();

        // 设置搜索方法
        //searchSourceBuilder.query(QueryBuilders.termQuery("name","spring cloud实战"));
        searchSourceBuilder.query(QueryBuilders.termQuery("description", "spring"));
        searchSourceBuilder.fetchSource(new String[]{"name", "price", "timestamp"}, new String[]{});

        // 请求对象设置 搜索源对象
        searchRequest.source(searchSourceBuilder);

        // 使用client  执行搜索
        SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);

        // 搜索结果
        SearchHits hits = searchResponse.getHits();

        // 匹配到的总记录数
        TotalHits totalHits = hits.getTotalHits();
        System.out.println("查询到的总记录数:" + totalHits.value);

        // 得到的匹配度高的文档
        SearchHit[] searchHits = hits.getHits();
        for (SearchHit hit : searchHits) {
            String id = hit.getId();
            // 源文档的内容
            Map<String, Object> sourceMap = hit.getSourceAsMap();
            String name = (String) sourceMap.get("name");
            String timestamp = (String) sourceMap.get("timestamp");
            String description = (String) sourceMap.get("description");
            Double price = (Double) sourceMap.get("price");
            System.out.println(name);
            System.out.println(timestamp);
            System.out.println(description);
            System.out.println(price);
        }
    }
}
```



#### （4）分页查询

```java
@SpringBootTest
@RunWith(SpringRunner.class)
public class d_querypage {
    @Autowired
    RestHighLevelClient client;

    @Test
    public void testSearchAllPage() throws IOException {
        // 搜索请求对象
        SearchRequest searchRequest = new SearchRequest("elasticsearch_test");

        // 搜索源构建对象
        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();

        // 设置搜索方法
        searchSourceBuilder.query(QueryBuilders.matchAllQuery());
        searchSourceBuilder.fetchSource(new String[]{"name", "price", "timestamp"}, new String[]{});

        // 设置分页参数
        int page = 2;
        int size = 2;

        // 计算出 from
        int form = (page - 1) * size;
        searchSourceBuilder.from(form);
        searchSourceBuilder.size(size);

        // 设置price 降序
        searchSourceBuilder.sort("price", SortOrder.DESC);

        // 请求对象设置 搜索源对象
        searchRequest.source(searchSourceBuilder);

        // 使用client  执行搜索
        SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);

        // 搜索结果
        SearchHits hits = searchResponse.getHits();

        // 匹配到的总记录数
        TotalHits totalHits = hits.getTotalHits();
        System.out.println("查询到的总记录数:" + totalHits.value);

        // 得到的匹配度高的文档
        SearchHit[] searchHits = hits.getHits();
        for (SearchHit hit : searchHits) {
            String id = hit.getId();

            // 源文档的内容
            Map<String, Object> sourceMap = hit.getSourceAsMap();
            String name = (String) sourceMap.get("name");
            String timestamp = (String) sourceMap.get("timestamp");
            String description = (String) sourceMap.get("description");
            Double price = (Double) sourceMap.get("price");
            System.out.println(name);
            System.out.println(timestamp);
            System.out.println(description);
            System.out.println(price);
        }
    }

    @Test
    public void testTermQueryPage() throws IOException {
        // 搜索请求对象
        SearchRequest searchRequest = new SearchRequest("elasticsearch_test");

        // 搜索源构建对象
        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();

        // 设置搜索方法
        searchSourceBuilder.query(QueryBuilders.termQuery("name", "spring cloud实战"));
        searchSourceBuilder.fetchSource(new String[]{"name", "price", "timestamp"}, new String[]{});

        // 设置分页参数
        int page = 1;
        int size = 2;

        // 计算出 from
        int form = (page - 1) * size;
        searchSourceBuilder.from(form);
        searchSourceBuilder.size(size);

        // 设置price 降序
        searchSourceBuilder.sort("price", SortOrder.DESC);

        // 请求对象设置 搜索源对象
        searchRequest.source(searchSourceBuilder);

        // 使用client  执行搜索
        SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);

        // 搜索结果
        SearchHits hits = searchResponse.getHits();

        // 匹配到的总记录数
        TotalHits totalHits = hits.getTotalHits();
        System.out.println("查询到的总记录数:" + totalHits.value);

        // 得到的匹配度高的文档
        SearchHit[] searchHits = hits.getHits();
        for (SearchHit hit : searchHits) {
            String id = hit.getId();
            // 源文档的内容
            Map<String, Object> sourceMap = hit.getSourceAsMap();
            String name = (String) sourceMap.get("name");
            String timestamp = (String) sourceMap.get("timestamp");
            String description = (String) sourceMap.get("description");
            Double price = (Double) sourceMap.get("price");
            System.out.println(name);
            System.out.println(timestamp);
            System.out.println(description);
            System.out.println(price);
        }
    }
}
```



# 第四章 ElasticSearch 集群



# 第四章 ElasticSearch 数据模型



# 第六章 ElasticSearch 原理



# 第七章 ElasticSearch 实战 